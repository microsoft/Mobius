<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.Spark.CSharp.Adapter</name>
    </assembly>
    <members>
        <member name="T:Microsoft.Spark.CSharp.Configuration.ConfigurationService">
            <summary>
            Implementation of configuration service that helps getting config settings
            to be used in SparkCLR runtime
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Configuration.IConfigurationService">
            <summary>
            Helps getting config settings to be used in SparkCLR runtime
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Configuration.IConfigurationService.GetCSharpWorkerExePath">
            <summary>
            The full path of the CSharp external backend worker process executable.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Configuration.IConfigurationService.BackendPortNumber">
            <summary>
            The port number used for communicating with the CSharp external backend worker process.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Configuration.ConfigurationService.SparkCLRConfiguration">
            <summary>
            Default configuration for SparkCLR jobs.
            Works with Standalone cluster mode
            May work with YARN or Mesos - needs validation when adding support for YARN/Mesos
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Configuration.ConfigurationService.SparkCLRConfiguration.GetPortNumber">
            <summary>
            The port number used for communicating with the CSharp external backend worker process.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Configuration.ConfigurationService.SparkCLRConfiguration.GetCSharpWorkerExePath">
            <summary>
            The path of the CSharp external backend worker process.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Configuration.ConfigurationService.SparkCLRLocalConfiguration">
            <summary>
            Configuration for SparkCLR jobs in ** Local ** mode
            Needs some investigation to find out why Local mode behaves
            different than standalone cluster mode for the configuration values
            overridden here
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Configuration.ConfigurationService.SparkCLRDebugConfiguration">
            <summary>
            Configuration mode for debug mode
            This configuration exists only to make SparkCLR development and debugging easier
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Configuration.ConfigurationService.SparkCLRDebugConfiguration.GetCSharpWorkerExePath">
            <summary>
            The full path of the CSharp external backend worker process.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Configuration.RunMode">
            <summary>
            The running mode used by Configuration Service
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Configuration.RunMode.UNKNOWN">
            <summary>
            Unknown running mode
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Configuration.RunMode.DEBUG">
            <summary>
            Debug mode, not a Spark mode but exists for develop debugging purpose
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Configuration.RunMode.LOCAL">
            <summary>
            Indicates service is running in local
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Configuration.RunMode.CLUSTER">
            <summary>
            Indicates service is running in cluster
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Configuration.RunMode.YARN">
            <summary>
            Indicates service is running in Yarn
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.Accumulator">
            <summary>
            A shared variable that can be accumulated, i.e., has a commutative and associative "add"
            operation. Worker tasks on a Spark cluster can add values to an Accumulator with the +=
            operator, but only the driver program is allowed to access its value, using Value.
            Updates from the workers get propagated automatically to the driver program.
            
            While <see cref="T:Microsoft.Spark.CSharp.Core.SparkContext"/> supports accumulators for primitive data types like int and
            float, users can also define accumulators for custom types by providing a custom
            <see cref="T:Microsoft.Spark.CSharp.Core.AccumulatorParam`1"/> object. Refer to the doctest of this module for an example.
            
            See python implementation in accumulators.py, worker.py, PythonRDD.scala
            
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Core.Accumulator.accumulatorId">
            <summary>
            The identity of the accumulator 
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Core.Accumulator.isDriver">
            <summary>
            Indicates whether the accumulator is on driver side.
            When deserialized on worker side, isDriver is false by default.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.Accumulator`1">
            <summary>
            A generic version of <see cref="T:Microsoft.Spark.CSharp.Core.Accumulator"/> where the element type is specified by the driver program.
            </summary>
            <typeparam name="T">The type of element in the accumulator.</typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Accumulator`1.#ctor(System.Int32,`0)">
            <summary>
            Initializes a new instance of the Accumulator class with a specified identity and a value.
            </summary>
            <param name="accumulatorId">The Identity of the accumulator</param>
            <param name="value">The value of the accumulator</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Accumulator`1.Add(`0)">
            <summary>
            Adds a term to this accumulator's value
            </summary>
            <param name="term"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Accumulator`1.op_Addition(Microsoft.Spark.CSharp.Core.Accumulator{`0},`0)">
            <summary>
            The += operator; adds a term to this accumulator's value
            </summary>
            <param name="self"></param>
            <param name="term"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Accumulator`1.ToString">
            <summary>
            Creates and returns a string representation of the current accumulator
            </summary>
            <returns>A string representation of the current accumulator</returns>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.Accumulator`1.Value">
            <summary>
            Gets or sets the value of the accumulator; only usable in driver program
            </summary>
            <exception cref="T:System.ArgumentException"></exception>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.AccumulatorParam`1">
            <summary>
            An AccumulatorParam that uses the + operators to add values. Designed for simple types
            such as integers, floats, and lists. Requires the zero value for the underlying type
            as a parameter.
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.AccumulatorParam`1.Zero(`0)">
            <summary>
            Provide a "zero value" for the type
            </summary>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.AccumulatorParam`1.AddInPlace(`0,`0)">
            <summary>
            Add two values of the accumulator's data type, returning a new value;
            </summary>
            <param name="value1"></param>
            <param name="value2"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.AccumulatorServer">
            <summary>
            A simple TCP server that intercepts shutdown() in order to interrupt
            our continuous polling on the handler.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.Broadcast">
            <summary>
            A broadcast variable created with SparkContext.Broadcast().
            Access its value through Value.
            
            var b = sc.Broadcast(new int[] {1, 2, 3, 4, 5})
            b.Value
            [1, 2, 3, 4, 5]
            sc.Parallelize(new in[] {0, 0}).FlatMap(x: b.Value).Collect()
            [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]
            b.Unpersist()
            
            See python implementation in broadcast.py, worker.py, PythonRDD.scala
            
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Core.Broadcast.broadcastRegistry">
            <summary>
            A thread-safe static collection that is used to store registered broadcast objects.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Broadcast.#ctor(System.String)">
            <summary>
            Initializes a new instance of Broadcast class with a specified path.
            </summary>
            <param name="path">The path that to be set.</param>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.Broadcast`1">
            <summary>
            A generic version of <see cref="T:Microsoft.Spark.CSharp.Core.Broadcast"/> where the element can be specified.
            </summary>
            <typeparam name="T">The type of element in Broadcast</typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Broadcast`1.Unpersist(System.Boolean)">
            <summary>
            Delete cached copies of this broadcast on the executors.
            </summary>
            <param name="blocking"></param>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.Broadcast`1.Value">
            <summary>
            Return the broadcasted value
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.Option`1">
            <summary>
            Container for an optional value of type T. If the value of type T is present, the Option.IsDefined is TRUE and GetValue() return the value. 
            If the value is absent, the Option.IsDefined is FALSE, exception will be thrown when calling GetValue().
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Option`1.#ctor">
            <summary>
            Initialize a instance of Option class without any value.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Option`1.#ctor(`0)">
            <summary>
            Initializes a instance of Option class with a specific value. 
            </summary>
            <param name="value">The value to be associated with the new instance.</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Option`1.GetValue">
            <summary>
            Returns the value of the option if Option.IsDefined is TRUE;
            otherwise, throws an <see cref="T:System.ArgumentException"/>.
            </summary>
            <returns></returns>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.Option`1.IsDefined">
            <summary>
            Indicates whether the option value is defined.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.Partitioner">
            <summary>
            An object that defines how the elements in a key-value pair RDD are partitioned by key.
            Maps each key to a partition ID, from 0 to "numPartitions - 1".
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.Partitioner.#ctor(System.Int32,System.Func{System.Object,System.Int32})">
            <summary>
            Create a <seealso cref="T:Microsoft.Spark.CSharp.Core.Partitioner"/> instance.
            </summary>
            <param name="numPartitions">Number of partitions.</param>
            <param name="partitionFunc">Defines how the elements in a key-value pair RDD are partitioned by key. Input of Func is key, output is partition index.
            Warning: diffrent Func instances are considered as different partitions which will cause repartition.</param>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.RDDCollector">
            <summary>
            Used for collect operation on RDD
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.IRDDCollector">
            <summary>
            Interface for collect operation on RDD
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions">
            <summary>
            Extra functions available on RDDs of Doubles through an implicit conversion. 
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.Sum(Microsoft.Spark.CSharp.Core.RDD{System.Double})">
            <summary>
            Add up the elements in this RDD.
            
            sc.Parallelize(new double[] {1.0, 2.0, 3.0}).Sum()
            6.0
            
            </summary>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.Stats(Microsoft.Spark.CSharp.Core.RDD{System.Double})">
            <summary>
            Return a <see cref="T:Microsoft.Spark.CSharp.Core.StatCounter"/> object that captures the mean, variance
            and count of the RDD's elements in one operation.
            </summary>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.Histogram(Microsoft.Spark.CSharp.Core.RDD{System.Double},System.Int32)">
            <summary>
            Compute a histogram using the provided buckets. The buckets
            are all open to the right except for the last which is closed.
            e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
            which means 1&lt;=x&lt;10, 10&lt;=x&lt;20, 20&lt;=x&lt;=50. And on the input of 1
            and 50 we would have a histogram of 1,0,1.
            
            If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
            this can be switched from an O(log n) inseration to O(1) per
            element(where n = # buckets).
            
            Buckets must be sorted and not contain any duplicates, must be
            at least two elements.
            
            If `buckets` is a number, it will generates buckets which are
            evenly spaced between the minimum and maximum of the RDD. For
            example, if the min value is 0 and the max is 100, given buckets
            as 2, the resulting buckets will be [0,50) [50,100]. buckets must
            be at least 1 If the RDD contains infinity, NaN throws an exception
            If the elements in RDD do not vary (max == min) always returns
            a single bucket.
            
            It will return an tuple of buckets and histogram.
            
            >>> rdd = sc.parallelize(range(51))
            >>> rdd.histogram(2)
            ([0, 25, 50], [25, 26])
            >>> rdd.histogram([0, 5, 25, 50])
            ([0, 5, 25, 50], [5, 20, 26])
            >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
            ([0, 15, 30, 45, 60], [15, 15, 15, 6])
            >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])
            >>> rdd.histogram(("a", "b", "c"))
            (('a', 'b', 'c'), [2, 2])
            
            </summary>
            <param name="self"></param>
            <param name="bucketCount"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.Mean(Microsoft.Spark.CSharp.Core.RDD{System.Double})">
            <summary>
            Compute the mean of this RDD's elements.
            sc.Parallelize(new double[]{1, 2, 3}).Mean()
            2.0
            </summary>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.Variance(Microsoft.Spark.CSharp.Core.RDD{System.Double})">
            <summary>
            Compute the variance of this RDD's elements.
            sc.Parallelize(new double[]{1, 2, 3}).Variance()
            0.666...
            </summary>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.Stdev(Microsoft.Spark.CSharp.Core.RDD{System.Double})">
            <summary>
            Compute the standard deviation of this RDD's elements.
            sc.Parallelize(new double[]{1, 2, 3}).Stdev()
            0.816...
            </summary>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.SampleStdev(Microsoft.Spark.CSharp.Core.RDD{System.Double})">
            <summary>
            Compute the sample standard deviation of this RDD's elements (which
            corrects for bias in estimating the standard deviation by dividing by
            N-1 instead of N).
            
            sc.Parallelize(new double[]{1, 2, 3}).SampleStdev()
            1.0
            
            </summary>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.DoubleRDDFunctions.SampleVariance(Microsoft.Spark.CSharp.Core.RDD{System.Double})">
            <summary>
            Compute the sample variance of this RDD's elements (which corrects
            for bias in estimating the variance by dividing by N-1 instead of N).
            
            sc.Parallelize(new double[]{1, 2, 3}).SampleVariance()
            1.0
            
            </summary>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.OrderedRDDFunctions">
            <summary>
            Extra functions available on RDDs of (key, value) pairs where the key is sortable through
            a function to sort the key.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.OrderedRDDFunctions.SortByKey``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Boolean,System.Nullable{System.Int32})">
            <summary>
            Sorts this RDD, which is assumed to consist of KeyValuePair pairs.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="ascending"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.OrderedRDDFunctions.SortByKey``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Boolean,System.Nullable{System.Int32},System.Func{``0,``2})">
            <summary>
            Sorts this RDD, which is assumed to consist of KeyValuePairs. If key is type of string, case is sensitive.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="U"></typeparam>
            <param name="self"></param>
            <param name="ascending"></param>
            <param name="numPartitions">Number of partitions. Each partition of the sorted RDD contains a sorted range of the elements.</param>
            <param name="keyFunc">RDD will sort by keyFunc(key) for every key in KeyValuePair. Must not be null.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.OrderedRDDFunctions.repartitionAndSortWithinPartitions``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Nullable{System.Int32},System.Func{``0,System.Int32},System.Boolean)">
             <summary>
             Repartition the RDD according to the given partitioner and, within each resulting partition,
             sort records by their keys.
            
             This is more efficient than calling `repartition` and then sorting within each partition
             because it can push the sorting down into the shuffle machinery.
             </summary>
             <typeparam name="K"></typeparam>
             <typeparam name="V"></typeparam>
             <param name="self"></param>
             <param name="numPartitions"></param>
             <param name="partitionFunc"></param>
             <param name="ascending"></param>
             <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.PairRDDFunctions">
            <summary>
            operations only available to KeyValuePair RDD
            
            See also http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.CollectAsMap``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}})">
             <summary>
             Return the key-value pairs in this RDD to the master as a dictionary.
            
             var m = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(1, 2), new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(3, 4) }, 1).CollectAsMap()
             m[1]
             2
             m[3]
             4
             
             </summary>
             <typeparam name="K"></typeparam>
             <typeparam name="V"></typeparam>
             <param name="self"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.Keys``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}})">
             <summary>
             Return an RDD with the keys of each tuple.
            
             &gt;&gt;&gt; m = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(1, 2), new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(3, 4) }, 1).Keys().Collect()
             [1, 3]
             </summary>
             <typeparam name="K"></typeparam>
             <typeparam name="V"></typeparam>
             <param name="self"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.Values``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}})">
             <summary>
             Return an RDD with the values of each tuple.
            
             &gt;&gt;&gt; m = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(1, 2), new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(3, 4) }, 1).Values().Collect()
             [2, 4]
             
             </summary>
             <typeparam name="K"></typeparam>
             <typeparam name="V"></typeparam>
             <param name="self"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.ReduceByKey``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,``1,``1},System.Int32)">
            <summary>
            Merge the values for each key using an associative reduce function.
            
            This will also perform the merging locally on each mapper before
            sending results to a reducer, similarly to a "combiner" in MapReduce.
            
            Output will be hash-partitioned with <paramref name="numPartitions"/> partitions, or
            the default parallelism level if <paramref name="numPartitions"/> is not specified.
            
            sc.Parallelize(new[] 
            { 
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), 
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 1),
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1)
            }, 2)
            .ReduceByKey((x, y) =&gt; x + y).Collect()
                   
            [('a', 2), ('b', 1)]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="reduceFunc"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.ReduceByKeyLocally``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,``1,``1})">
            <summary>
            Merge the values for each key using an associative reduce function, but
            return the results immediately to the master as a dictionary.
            
            This will also perform the merging locally on each mapper before
            sending results to a reducer, similarly to a "combiner" in MapReduce.
            
            sc.Parallelize(new[] 
            { 
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), 
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 1),
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1)
            }, 2)
            .ReduceByKeyLocally((x, y) =&gt; x + y).Collect()
            
            [('a', 2), ('b', 1)]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="reduceFunc"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.CountByKey``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}})">
            <summary>
            Count the number of elements for each key, and return the result to the master as a dictionary.
            
            sc.Parallelize(new[] 
            { 
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), 
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 1),
                new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1)
            }, 2)
            .CountByKey((x, y) =&gt; x + y).Collect()
            
            [('a', 2), ('b', 1)]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.Join``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Return an RDD containing all pairs of elements with matching keys in this RDD and <paramref name="other"/>.
            
            Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this RDD and (k, v2) is in <paramref name="other"/>.
            
            Performs a hash join across the cluster.
            
            var l = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 4) }, 1);
            var r = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 2), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 3) }, 1);
            var m = l.Join(r, 2).Collect();
            
            [('a', (1, 2)), ('a', (1, 3))]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.LeftOuterJoin``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Perform a left outer join of this RDD and <paramref name="other"/>.
            
            For each element (k, v) in this RDD, the resulting RDD will either
            contain all pairs (k, (v, Option)) for w in <paramref name="other"/>, where Option.IsDefined is TRUE, or the pair
            (k, (v, Option)) if no elements in <paramref name="other"/> have key k, where Option.IsDefined is FALSE. 
            
            Hash-partitions the resulting RDD into the given number of partitions.
            
            var l = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 4) }, 1);
            var r = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 2) }, 1);
            var m = l.LeftOuterJoin(r).Collect();
            
            [('a', (1, 2)), ('b', (4, Option))]
            * Option.IsDefined = FALSE
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.RightOuterJoin``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Perform a right outer join of this RDD and <paramref name="other"/>.
            
            For each element (k, w) in <paramref name="other"/>, the resulting RDD will either
            contain all pairs (k, (Option, w)) for v in this, where Option.IsDefined is TRUE, or the pair (k, (Option, w))
            if no elements in this RDD have key k, where Option.IsDefined is FALSE.
            
            Hash-partitions the resulting RDD into the given number of partitions.
            
            var l = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 2) }, 1);
            var r = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 4) }, 1);
            var m = l.RightOuterJoin(r).Collect();
            
            [('a', (2, 1)), ('b', (Option, 4))]
            * Option.IsDefined = FALSE
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.FullOuterJoin``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Perform a full outer join of this RDD and <paramref name="other"/>.
            
            For each element (k, v) in this RDD, the resulting RDD will either
            contain all pairs (k, (v, w)) for w in <paramref name="other"/>, or the pair
            (k, (v, None)) if no elements in <paramref name="other"/> have key k.
            
            Similarly, for each element (k, w) in <paramref name="other"/>, the resulting RDD will
            either contain all pairs (k, (v, w)) for v in this RDD, or the pair
            (k, (None, w)) if no elements in this RDD have key k.
            
            Hash-partitions the resulting RDD into the given number of partitions.
            
            var l = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 4) }, 1);
            var r = sc.Parallelize(
                new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 2), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("c", 8) }, 1);
            var m = l.FullOuterJoin(r).Collect();
            
            [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.PartitionBy``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Int32,System.Func{System.Object,System.Int32})">
            <summary>
            Return a copy of the RDD partitioned using the specified partitioner.
            
            sc.Parallelize(new[] { 1, 2, 3, 4, 2, 4, 1 }, 1).Map(x =&gt; new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(x, x)).PartitionBy(3).Glom().Collect()
            </summary>
            <param name="self"></param>
            <param name="numPartitions"></param>
            <param name="partitionFunc"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.CombineByKey``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``2},System.Func{``2,``1,``2},System.Func{``2,``2,``2},System.Int32)">
            <summary>
            # TODO: add control over map-side aggregation
            Generic function to combine the elements for each key using a custom
            set of aggregation functions.
            
            Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined
            type" C.  Note that V and C can be different -- for example, one might
            group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).
            
            Users provide three functions:
            
                - <paramref name="createCombiner"/>, which turns a V into a C (e.g., creates a one-element list)
                - <paramref name="mergeValue"/>, to merge a V into a C (e.g., adds it to the end of
                  a list)
                - <paramref name="mergeCombiners"/>, to combine two C's into a single one.
            
            In addition, users can control the partitioning of the output RDD.
            
            sc.Parallelize(
                    new[] 
                    { 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 1),
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1)
                    }, 2)
                    .CombineByKey(() =&gt; string.Empty, (x, y) =&gt; x + y.ToString(), (x, y) =&gt; x + y).Collect()
                    
            [('a', '11'), ('b', '1')]
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="C"></typeparam>
            <param name="self"></param>
            <param name="createCombiner"></param>
            <param name="mergeValue"></param>
            <param name="mergeCombiners"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.AggregateByKey``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``2},System.Func{``2,``1,``2},System.Func{``2,``2,``2},System.Int32)">
            <summary>
            Aggregate the values of each key, using given combine functions and a neutral
            "zero value". This function can return a different result type, U, than the type
            of the values in this RDD, V. Thus, we need one operation for merging a V into
            a U and one operation for merging two U's, The former operation is used for merging
            values within a partition, and the latter is used for merging values between
            partitions. To avoid memory allocation, both of these functions are
            allowed to modify and return their first argument instead of creating a new U.
            
            sc.Parallelize(
                    new[] 
                    { 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 1),
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1)
                    }, 2)
                    .CombineByKey(() =&gt; string.Empty, (x, y) =&gt; x + y.ToString(), (x, y) =&gt; x + y).Collect()
                    
            [('a', 2), ('b', 1)]
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="U"></typeparam>
            <param name="self"></param>
            <param name="zeroValue"></param>
            <param name="seqOp"></param>
            <param name="combOp"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.FoldByKey``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1},System.Func{``1,``1,``1},System.Int32)">
            <summary>
            Merge the values for each key using an associative function "func"
            and a neutral "zeroValue" which may be added to the result an
            arbitrary number of times, and must not change the result
            (e.g., 0 for addition, or 1 for multiplication.).
            
            sc.Parallelize(
                    new[] 
                    { 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 1),
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1)
                    }, 2)
                    .CombineByKey(() =&gt; string.Empty, (x, y) =&gt; x + y.ToString(), (x, y) =&gt; x + y).Collect()
                    
            [('a', 2), ('b', 1)]
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="zeroValue"></param>
            <param name="func"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.GroupByKey``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Int32)">
            <summary>
            Group the values for each key in the RDD into a single sequence.
            Hash-partitions the resulting RDD with numPartitions partitions.
            
            Note: If you are grouping in order to perform an aggregation (such as a
            sum or average) over each key, using reduceByKey or aggregateByKey will
            provide much better performance.
            
            sc.Parallelize(
                    new[] 
                    { 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), 
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 1),
                        new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1)
                    }, 2)
                    .GroupByKey().MapValues(l =&gt; string.Join(" ", l)).Collect()
                    
            [('a', [1, 1]), ('b', [1])]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.MapValues``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,``2})">
            <summary>
            Pass each value in the key-value pair RDD through a map function
            without changing the keys; this also retains the original RDD's partitioning.
            
            sc.Parallelize(
                    new[] 
                    { 
                        new <see cref="!:KeyValuePair&lt;string, string[]&gt;"/>("a", new[]{"apple", "banana", "lemon"}), 
                        new <see cref="!:KeyValuePair&lt;string, string[]&gt;"/>("b", new[]{"grapes"})
                    }, 2)
                    .MapValues(x =&gt; x.Length).Collect()
                    
            [('a', 3), ('b', 1)]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="U"></typeparam>
            <param name="self"></param>
            <param name="func"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.FlatMapValues``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,System.Collections.Generic.IEnumerable{``2}})">
            <summary>
            Pass each value in the key-value pair RDD through a flatMap function
            without changing the keys; this also retains the original RDD's partitioning.
            
            x = sc.Parallelize(
                    new[] 
                    { 
                        new <see cref="!:KeyValuePair&lt;string, string[]&gt;"/>("a", new[]{"x", "y", "z"}), 
                        new <see cref="!:KeyValuePair&lt;string, string[]&gt;"/>("b", new[]{"p", "r"})
                    }, 2)
                    .FlatMapValues(x =&gt; x).Collect()
                    
            [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="U"></typeparam>
            <param name="self"></param>
            <param name="func"></param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.MapPartitionsWithIndex``5(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,System.Object}})" -->
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.GroupWith``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            For each key k in this RDD or <paramref name="other"/>, return a resulting RDD that
            contains a tuple with the list of values for that key in this RDD as well as <paramref name="other"/>.
            
            var x = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 4) }, 2);
            var y = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 2) }, 1);
            x.GroupWith(y).Collect();
            
            [('a', ([1], [2])), ('b', ([4], []))]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.GroupWith``4(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``3}},System.Int32)">
            <summary>
            var x = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 5), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 6) }, 2);
            var y = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 4) }, 2);
            var z = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 2) }, 1);
            x.GroupWith(y, z).Collect();
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W1"></typeparam>
            <typeparam name="W2"></typeparam>
            <param name="self"></param>
            <param name="other1"></param>
            <param name="other2"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.GroupWith``5(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``3}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``4}},System.Int32)">
            <summary>
            var x = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 5), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 6) }, 2);
            var y = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 1), new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 4) }, 2);
            var z = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("a", 2) }, 1);
            var w = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int&gt;"/>("b", 42) }, 1);
            var m = x.GroupWith(y, z, w).MapValues(l =&gt; string.Join(" ", l.Item1) + " : " + string.Join(" ", l.Item2) + " : " + string.Join(" ", l.Item3) + " : " + string.Join(" ", l.Item4)).Collect();
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W1"></typeparam>
            <typeparam name="W2"></typeparam>
            <typeparam name="W3"></typeparam>
            <param name="self"></param>
            <param name="other1"></param>
            <param name="other2"></param>
            <param name="other3"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.SubtractByKey``3(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Return each (key, value) pair in this RDD that has no pair with matching key in <paramref name="other"/>.
            
            var x = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int?&gt;"/>("a", 1), new <see cref="!:KeyValuePair&lt;string, int?&gt;"/>("b", 4), new <see cref="!:KeyValuePair&lt;string, int?&gt;"/>("b", 5), new <see cref="!:KeyValuePair&lt;string, int?&gt;"/>("a", 2) }, 2);
            var y = sc.Parallelize(new[] { new <see cref="!:KeyValuePair&lt;string, int?&gt;"/>("a", 3), new <see cref="!:KeyValuePair&lt;string, int?&gt;"/>("c", null) }, 2);
            x.SubtractByKey(y).Collect();
            
            [('b', 4), ('b', 5)]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.Lookup``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},``0)">
            <summary>
            Return the list of values in the RDD for key `key`. This operation
            is done efficiently if the RDD has a known partitioner by only
            searching the partition that the key maps to.
            
            &gt;&gt;&gt; l = range(1000)
            &gt;&gt;&gt; rdd = sc.Parallelize(Enumerable.Range(0, 1000).Zip(Enumerable.Range(0, 1000), (x, y) =&gt; new <see cref="!:KeyValuePair&lt;int, int&gt;"/>(x, y)), 10)
            &gt;&gt;&gt; rdd.lookup(42)
            [42]
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="key"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.SaveAsNewAPIHadoopDataset``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}})">
            <summary>
            Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file
            system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
            converted for output using either user specified converters or, by default,
            org.apache.spark.api.python.JavaToWritableConverter.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="conf">Hadoop job configuration, passed in as a dict</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.SaveAsNewAPIHadoopFile``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.String,System.String,System.String,System.String,System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}})">
            <summary>
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="path">path to Hadoop file</param>
            <param name="outputFormatClass">fully qualified classname of Hadoop OutputFormat (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")</param>
            <param name="keyClass">fully qualified classname of key Writable class (e.g. "org.apache.hadoop.io.IntWritable", None by default)</param>
            <param name="valueClass">fully qualified classname of value Writable class (e.g. "org.apache.hadoop.io.Text", None by default)</param>
            <param name="conf">Hadoop job configuration, passed in as a dict (None by default)</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.SaveAsHadoopDataset``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}})">
            <summary>
            Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file
            system, using the old Hadoop OutputFormat API (mapred package). Keys/values are
            converted for output using either user specified converters or, by default,
            org.apache.spark.api.python.JavaToWritableConverter.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="conf">Hadoop job configuration, passed in as a dict</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.SaveAsHadoopFile``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.String,System.String,System.String,System.String,System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}},System.String)">
            <summary>
            Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file
            system, using the old Hadoop OutputFormat API (mapred package). Key and value types
            will be inferred if not specified. Keys and values are converted for output using either
            user specified converters or org.apache.spark.api.python.JavaToWritableConverter. The
            <paramref name="conf"/> is applied on top of the base Hadoop conf associated with the SparkContext
            of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="path">path to Hadoop file</param>
            <param name="outputFormatClass">fully qualified classname of Hadoop OutputFormat (e.g. "org.apache.hadoop.mapred.SequenceFileOutputFormat")</param>
            <param name="keyClass">fully qualified classname of key Writable class (e.g. "org.apache.hadoop.io.IntWritable", None by default)</param>
            <param name="valueClass">fully qualified classname of value Writable class (e.g. "org.apache.hadoop.io.Text", None by default)</param>
            <param name="conf">(None by default)</param>
            <param name="compressionCodecClass">(None by default)</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.PairRDDFunctions.SaveAsSequenceFile``2(Microsoft.Spark.CSharp.Core.RDD{System.Collections.Generic.KeyValuePair{``0,``1}},System.String,System.String)">
            <summary>
            Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file
            system, using the org.apache.hadoop.io.Writable types that we convert from the
            RDD's key and value types. The mechanism is as follows:
            
                1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
                2. Keys and values of this Java RDD are converted to Writables and written out.
            
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="path">path to sequence file</param>
            <param name="compressionCodecClass">(None by default)</param>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.PairRDDFunctions.GroupByMergeHelper`2">
            <summary>
            These classes are defined explicitly and marked as [Serializable]instead of using anonymous method as delegate to 
            prevent C# compiler from generating private anonymous type that is not serializable. Since the delegate has to be 
            serialized and sent to the Spark workers for execution, it is necessary to have the type marked [Serializable]. 
            These classes are to work around the limitation on the serializability of compiler generated types
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.Spark.CSharp.Core.PipelinedRDD`1" -->
        <member name="T:Microsoft.Spark.CSharp.Core.RDD`1">
            <summary>
            Represents a Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, 
            partitioned collection of elements that can be operated on in parallel
            
            See also http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD
            </summary>
            <typeparam name="T">Type of the RDD</typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Cache">
            <summary>
            Persist this RDD with the default storage level <see cref="F:Microsoft.Spark.CSharp.Core.StorageLevelType.MEMORY_ONLY_SER"/>.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Persist(Microsoft.Spark.CSharp.Core.StorageLevelType)">
            <summary>
            Set this RDD's storage level to persist its values across operations
            after the first time it is computed. This can only be used to assign
            a new storage level if the RDD does not have a storage level set yet.
            If no storage level is specified defaults to <see cref="F:Microsoft.Spark.CSharp.Core.StorageLevelType.MEMORY_ONLY_SER"/>.
            
            sc.Parallelize(new string[] {"b", "a", "c").Persist().isCached
            True
            
            </summary>
            <param name="storageLevelType"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Unpersist">
            <summary>
            Mark the RDD as non-persistent, and remove all blocks for it from
            memory and disk.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Checkpoint">
            <summary>
            Mark this RDD for checkpointing. It will be saved to a file inside the
            checkpoint directory set with <see cref="M:Microsoft.Spark.CSharp.Core.SparkContext.SetCheckpointDir(System.String)"/>) and
            all references to its parent RDDs will be removed. This function must
            be called before any job has been executed on this RDD. It is strongly
            recommended that this RDD is persisted in memory, otherwise saving it
            on a file will require recomputation.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Map``1(System.Func{`0,``0},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each element of this RDD.
            
            sc.Parallelize(new string[]{"b", "a", "c"}, 1).Map(x =&gt; new <see cref="!:KeyValuePair&lt;string, int&gt;"/>(x, 1)).Collect()
            [('a', 1), ('b', 1), ('c', 1)]
            
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <param name="preservesPartitioning"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.FlatMap``1(System.Func{`0,System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by first applying a function to all elements of this
            RDD, and then flattening the results.
            
            sc.Parallelize(new int[] {2, 3, 4}, 1).FlatMap(x => Enumerable.Range(1, x - 1)).Collect()
            [1, 1, 1, 2, 2, 3]
            
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <param name="preservesPartitioning"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.MapPartitions``1(System.Func{System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD.
            
            sc.Parallelize(new int[] {1, 2, 3, 4}, 2).MapPartitions(iter => new[]{iter.Sum(x => (x as decimal?))}).Collect()
            [3, 7]
            
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <param name="preservesPartitioning"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.MapPartitionsWithIndex``1(System.Func{System.Int32,System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD,
            while tracking the index of the original partition.
            
            <see cref="!:sc.Parallelize(new int[]&lt;1, 2, 3, 4&gt;, 4).MapPartitionsWithIndex&lt;double&gt;"/>((pid, iter) =&gt; (double)pid).Sum()
            6
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <param name="preservesPartitioningParam"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Filter(System.Func{`0,System.Boolean})">
            <summary>
            Return a new RDD containing only the elements that satisfy a predicate.
            
            sc.Parallelize(new int[]{1, 2, 3, 4, 5}, 1).Filter(x => x % 2 == 0).Collect()
            [2, 4]
            </summary>
            <param name="f"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Distinct(System.Int32)">
            <summary>
            Return a new RDD containing the distinct elements in this RDD.
            
            >>> sc.Parallelize(new int[] {1, 1, 2, 3}, 1).Distinct().Collect()
            [1, 2, 3]
            </summary>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Sample(System.Boolean,System.Double,System.Int64)">
             <summary>
             Return a sampled subset of this RDD.
            
             var rdd = sc.Parallelize(Enumerable.Range(0, 100), 4)
             6 &lt;= rdd.Sample(False, 0.1, 81).count() &lt;= 14
             true
             
             </summary>
             <param name="withReplacement">can elements be sampled multiple times (replaced when sampled out)</param>
             <param name="fraction">expected size of the sample as a fraction of this RDD's size
                without replacement: probability that each element is chosen; fraction must be [0, 1]
                with replacement: expected number of times each element is chosen; fraction must be >= 0</param>
             <param name="seed">seed for the random number generator</param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.RandomSplit(System.Double[],System.Int64)">
             <summary>
             Randomly splits this RDD with the provided weights.
            
             var rdd = sc.Parallelize(Enumerable.Range(0, 500), 1)
             var rdds = rdd.RandomSplit(new double[] {2, 3}, 17)
             150 &lt; rdds[0].Count() &lt; 250
             250 &lt; rdds[1].Count() &lt; 350
             
             </summary>
             <param name="weights">weights for splits, will be normalized if they don't sum to 1</param>
             <param name="seed">random seed</param>
             <returns>split RDDs in a list</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.TakeSample(System.Boolean,System.Int32,System.Int32)">
            <summary>
            Return a fixed-size sampled subset of this RDD.
            
            var rdd = sc.Parallelize(Enumerable.Range(0, 10), 2)
            rdd.TakeSample(true, 20, 1).Length
            20
            rdd.TakeSample(false, 5, 2).Length
            5
            rdd.TakeSample(false, 15, 3).Length
            10
            
            </summary>
            <param name="withReplacement"></param>
            <param name="num"></param>
            <param name="seed"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.ComputeFractionForSampleSize(System.Int32,System.Int32,System.Boolean)">
            <summary>
            Returns a sampling rate that guarantees a sample of
            size >= sampleSizeLowerBound 99.99% of the time.
            
            How the sampling rate is determined:
            Let p = num / total, where num is the sample size and total is the
            total number of data points in the RDD. We're trying to compute
            q > p such that
              - when sampling with replacement, we're drawing each data point
                with prob_i ~ Pois(q), where we want to guarantee
                Pr[s &lt; num] &lt; 0.0001 for s = sum(prob_i for i from 0 to
                total), i.e. the failure rate of not having a sufficiently large
                sample &lt; 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
                to guarantee 0.9999 success rate for num > 12, but we need a
                slightly larger q (9 empirically determined).
              - when sampling without replacement, we're drawing each data point
                with prob_i ~ Binomial(total, fraction) and our choice of q
                guarantees 1-delta, or 0.9999 success rate, where success rate is
                defined the same as in sampling with replacement.
            </summary>
            <param name="sampleSizeLowerBound"></param>
            <param name="total"></param>
            <param name="withReplacement"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Union(Microsoft.Spark.CSharp.Core.RDD{`0})">
            <summary>
            Return the union of this RDD and another one.
            
            var rdd = sc.Parallelize(new int[] { 1, 1, 2, 3 }, 1)
            rdd.union(rdd).collect()
            [1, 1, 2, 3, 1, 1, 2, 3]
            
            </summary>
            <param name="other"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Intersection(Microsoft.Spark.CSharp.Core.RDD{`0})">
            <summary>
            Return the intersection of this RDD and another one. The output will
            not contain any duplicate elements, even if the input RDDs did.
            
            Note that this method performs a shuffle internally.
            
            var rdd1 = sc.Parallelize(new int[] { 1, 10, 2, 3, 4, 5 }, 1)
            var rdd2 = sc.Parallelize(new int[] { 1, 6, 2, 3, 7, 8 }, 1)
            var rdd1.Intersection(rdd2).Collect()
            [1, 2, 3]
            
            </summary>
            <param name="other"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Glom">
             <summary>
             Return an RDD created by coalescing all elements within each partition into a list.
            
             var rdd = sc.Parallelize(new int[] { 1, 2, 3, 4 }, 2)
             rdd.Glom().Collect()
             [[1, 2], [3, 4]]
             
             </summary>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Cartesian``1(Microsoft.Spark.CSharp.Core.RDD{``0})">
            <summary>
            Return the Cartesian product of this RDD and another one, that is, the
            RDD of all pairs of elements (a, b) where a is in self and b is in other.
            
            rdd = sc.Parallelize(new int[] { 1, 2 }, 1)
            rdd.Cartesian(rdd).Collect()
            [(1, 1), (1, 2), (2, 1), (2, 2)]
            
            </summary>
            <typeparam name="U"></typeparam>
            <param name="other"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.GroupBy``1(System.Func{`0,``0},System.Int32)">
             <summary>
             Return an RDD of grouped items. Each group consists of a key and a sequence of elements
             mapping to that key. The ordering of elements within each group is not guaranteed, and
             may even differ each time the resulting RDD is evaluated.
            
             Note: This operation may be very expensive. If you are grouping in order to perform an
             aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]
             or [[PairRDDFunctions.reduceByKey]] will provide much better performance.
             
             >>> rdd = sc.Parallelize(new int[] { 1, 1, 2, 3, 5, 8 }, 1)
             >>> result = rdd.GroupBy(lambda x: x % 2).Collect()
             [(0, [2, 8]), (1, [1, 1, 3, 5])]
             
             </summary>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Pipe(System.String)">
             <summary>
             Return an RDD created by piping elements to a forked external process.
            
             >>> sc.Parallelize(new char[] { '1', '2', '3', '4' }, 1).Pipe("cat").Collect()
             [u'1', u'2', u'3', u'4']
             </summary>
             <param name="command"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Foreach(System.Action{`0})">
             <summary>
             Applies a function to all elements of this RDD.
            
             sc.Parallelize(new int[] { 1, 2, 3, 4, 5 }, 1).Foreach(x => Console.Write(x))
             
             </summary>
             <param name="f"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.ForeachPartition(System.Action{System.Collections.Generic.IEnumerable{`0}})">
             <summary>
             Applies a function to each partition of this RDD.
            
             sc.parallelize(new int[] { 1, 2, 3, 4, 5 }, 1).ForeachPartition(iter => { foreach (var x in iter) Console.Write(x + " "); })
             
             </summary>
             <param name="f"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Collect">
            <summary>
            Return a list that contains all of the elements in this RDD.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Reduce(System.Func{`0,`0,`0})">
            <summary>
            Reduces the elements of this RDD using the specified commutative and
            associative binary operator.
            
            sc.Parallelize(new int[] { 1, 2, 3, 4, 5 }, 1).Reduce((x, y) => x + y)
            15
            
            </summary>
            <param name="f"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.TreeReduce(System.Func{`0,`0,`0},System.Int32)">
            <summary>
            Reduces the elements of this RDD in a multi-level tree pattern.
            
            >>> add = lambda x, y: x + y
            >>> rdd = sc.Parallelize(new int[] { -5, -4, -3, -2, -1, 1, 2, 3, 4 }, 10).TreeReduce((x, y) => x + y))
            >>> rdd.TreeReduce(add)
            -5
            >>> rdd.TreeReduce(add, 1)
            -5
            >>> rdd.TreeReduce(add, 2)
            -5
            >>> rdd.TreeReduce(add, 5)
            -5
            >>> rdd.TreeReduce(add, 10)
            -5
            
            </summary>
            <param name="f"></param>
            <param name="depth">suggested depth of the tree (default: 2)</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Fold(`0,System.Func{`0,`0,`0})">
            <summary>
            Aggregate the elements of each partition, and then the results for all
            the partitions, using a given associative and commutative function and
            a neutral "zero value."
            
            The function op(t1, t2) is allowed to modify t1 and return it
            as its result value to avoid object allocation; however, it should not
            modify t2.
            
            This behaves somewhat differently from fold operations implemented
            for non-distributed collections in functional languages like Scala.
            This fold operation may be applied to partitions individually, and then
            fold those results into the final result, rather than apply the fold
            to each element sequentially in some defined ordering. For functions
            that are not commutative, the result may differ from that of a fold
            applied to a non-distributed collection.
            
            >>> from operator import add
            >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
            15
            
            </summary>
            <param name="zeroValue"></param>
            <param name="op"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Aggregate``1(``0,System.Func{``0,`0,``0},System.Func{``0,``0,``0})">
            <summary>
            Aggregate the elements of each partition, and then the results for all
            the partitions, using a given combine functions and a neutral "zero
            value."
            
            The functions op(t1, t2) is allowed to modify t1 and return it
            as its result value to avoid object allocation; however, it should not
            modify t2.
            
            The first function (seqOp) can return a different result type, U, than
            the type of this RDD. Thus, we need one operation for merging a T into
            an U and one operation for merging two U
            
            >>> sc.parallelize(new int[] { 1, 2, 3, 4 }, 1).Aggregate(0, (x, y) => x + y, (x, y) => x + y))
            10
            
            </summary>
            <typeparam name="U"></typeparam>
            <param name="zeroValue"></param>
            <param name="seqOp"></param>
            <param name="combOp"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.TreeAggregate``1(``0,System.Func{``0,`0,``0},System.Func{``0,``0,``0},System.Int32)">
            <summary>
            Aggregates the elements of this RDD in a multi-level tree pattern.
            
            rdd = sc.Parallelize(new int[] { 1, 2, 3, 4 }, 1).TreeAggregate(0, (x, y) => x + y, (x, y) => x + y))
            10
            </summary>
            <typeparam name="U"></typeparam>
            <param name="zeroValue"></param>
            <param name="seqOp"></param>
            <param name="combOp"></param>
            <param name="depth">suggested depth of the tree (default: 2)</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Count">
            <summary>
            Return the number of elements in this RDD.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.CountByValue">
            <summary>
            Return the count of each unique value in this RDD as a dictionary of
            (value, count) pairs.
            
            sc.Parallelize(new int[] { 1, 2, 1, 2, 2 }, 2).CountByValue())
            [(1, 2), (2, 3)]
            
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Take(System.Int32)">
            <summary>
            Take the first num elements of the RDD.
            
            It works by first scanning one partition, and use the results from
            that partition to estimate the number of additional partitions needed
            to satisfy the limit.
            
            Translated from the Scala implementation in RDD#take().
            
            sc.Parallelize(new int[] { 2, 3, 4, 5, 6 }, 2).Cache().Take(2)))
            [2, 3]
            sc.Parallelize(new int[] { 2, 3, 4, 5, 6 }, 2).Take(10)
            [2, 3, 4, 5, 6]
            sc.Parallelize(Enumerable.Range(0, 100), 100).Filter(x => x > 90).Take(3)
            [91, 92, 93]
            
            </summary>
            <param name="num"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.First">
            <summary>
            Return the first element in this RDD.
            
            >>> sc.Parallelize(new int[] { 2, 3, 4 }, 2).First()
            2
            
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.IsEmpty">
            <summary>
            Returns true if and only if the RDD contains no elements at all. Note that an RDD
            may be empty even when it has at least 1 partition.
            
            sc.Parallelize(new int[0], 1).isEmpty()
            true
            sc.Parallelize(new int[] {1}).isEmpty()
            false
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Subtract(Microsoft.Spark.CSharp.Core.RDD{`0},System.Int32)">
            <summary>
            Return each value in this RDD that is not contained in <paramref name="other"/>.
            
            var x = sc.Parallelize(new int[] { 1, 2, 3, 4 }, 1)
            var y = sc.Parallelize(new int[] { 3 }, 1)
            x.Subtract(y).Collect())
            [1, 2, 4]
            
            </summary>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.KeyBy``1(System.Func{`0,``0})">
            <summary>
            Creates tuples of the elements in this RDD by applying <paramref name="f"/>.
            
            sc.Parallelize(new int[] { 1, 2, 3, 4 }, 1).KeyBy(x => x * x).Collect())
            (1, 1), (4, 2), (9, 3), (16, 4)
            
            </summary>
            <typeparam name="K"></typeparam>
            <param name="f"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Repartition(System.Int32)">
             <summary>
             Return a new RDD that has exactly numPartitions partitions.
            
             Can increase or decrease the level of parallelism in this RDD.
             Internally, this uses a shuffle to redistribute data.
             If you are decreasing the number of partitions in this RDD, consider
             using `Coalesce`, which can avoid performing a shuffle.
            
             var rdd = sc.Parallelize(new int[] { 1, 2, 3, 4, 5, 6, 7 }, 4)
             rdd.Glom().Collect().Length
             4
             rdd.Repartition(2).Glom().Collect().Length
             2
             
             </summary>
             <param name="numPartitions"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Coalesce(System.Int32,System.Boolean)">
             <summary>
             Return a new RDD that is reduced into `numPartitions` partitions.
            
             sc.Parallelize(new int[] { 1, 2, 3, 4, 5 }, 3).Glom().Collect().Length
             3
             >>> sc.Parallelize(new int[] { 1, 2, 3, 4, 5 }, 3).Coalesce(1).Glom().Collect().Length
             1
             
             </summary>
             <param name="numPartitions"></param>
             <param name="shuffle"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.Zip``1(Microsoft.Spark.CSharp.Core.RDD{``0})">
            <summary>
            Zips this RDD with another one, returning key-value pairs with the
            first element in each RDD second element in each RDD, etc. Assumes
            that the two RDDs have the same number of partitions and the same
            number of elements in each partition (e.g. one was made through
            a map on the other).
            
            var x = sc.parallelize(range(0,5))
            var y = sc.parallelize(range(1000, 1005))
            x.Zip(y).Collect()
            [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]
            
            </summary>
            <typeparam name="U"></typeparam>
            <param name="other"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.ZipWithIndex">
            <summary>
            Zips this RDD with its element indices.
            
            The ordering is first based on the partition index and then the
            ordering of items within each partition. So the first item in
            the first partition gets index 0, and the last item in the last
            partition receives the largest index.
            
            This method needs to trigger a spark job when this RDD contains
            more than one partitions.
            
            sc.Parallelize(new string[] { "a", "b", "c", "d" }, 3).ZipWithIndex().Collect()
            [('a', 0), ('b', 1), ('c', 2), ('d', 3)]
            
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.ZipWithUniqueId">
            <summary>
            Zips this RDD with generated unique Long ids.
            
            Items in the kth partition will get ids k, n+k, 2*n+k, ..., where
            n is the number of partitions. So there may exist gaps, but this
            method won't trigger a spark job, which is different from <see cref="M:Microsoft.Spark.CSharp.Core.RDD`1.ZipWithIndex"/>
            
            &gt;&gt;&gt; sc.Parallelize(new string[] { "a", "b", "c", "d" }, 1).ZipWithIndex().Collect()
            [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]
            
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.SetName(System.String)">
            <summary>
            Assign a name to this RDD.
            
            >>> rdd1 = sc.parallelize([1, 2])
            >>> rdd1.setName('RDD1').name()
            u'RDD1'
            
            </summary>
            <param name="name"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.ToDebugString">
            <summary>
            A description of this RDD and its recursive dependencies for debugging.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.GetStorageLevel">
            <summary>
            Get the RDD's current storage level.
            
            >>> rdd1 = sc.parallelize([1,2])
            >>> rdd1.getStorageLevel()
            StorageLevel(False, False, False, False, 1)
            >>> print(rdd1.getStorageLevel())
            Serialized 1x Replicated
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.ToLocalIterator">
            <summary>
            Return an iterator that contains all of the elements in this RDD.
            The iterator will consume as much memory as the largest partition in this RDD.
            sc.Parallelize(Enumerable.Range(0, 10), 1).ToLocalIterator()
            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.RDD`1.RandomSampleWithRange(System.Double,System.Double,System.Int64)">
            <summary>
            Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability range.
            </summary>
            <param name="lb">lower bound to use for the Bernoulli sampler</param>
            <param name="ub">upper bound to use for the Bernoulli sampler</param>
            <param name="seed">the seed for the Random number generator</param>
            <returns>A random sub-sample of the RDD without replacement.</returns>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.RDD`1.IsCached">
            <summary>
            Return whether this RDD has been cached or not
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.RDD`1.IsCheckpointed">
            <summary>
            Return whether this RDD has been checkpointed or not
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.RDD`1.Name">
            <summary>
            Return the name of this RDD.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.PipelinedRDD`1.MapPartitionsWithIndexHelper`2">
            <summary>
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.StringRDDFunctions">
            <summary>
            Some useful utility functions for <c>RDD{string}</c>
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StringRDDFunctions.SaveAsTextFile(Microsoft.Spark.CSharp.Core.RDD{System.String},System.String,System.String)">
            <summary>
            Save this RDD as a text file, using string representations of elements.
            </summary>
            <param name="self"></param>
            <param name="path">path to text file</param>
            <param name="compressionCodecClass">(None by default) string i.e. "org.apache.hadoop.io.compress.GzipCodec"</param>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.ComparableRDDFunctions">
            <summary>
            Some useful utility functions for RDD's containing IComparable values.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.ComparableRDDFunctions.Max``1(Microsoft.Spark.CSharp.Core.RDD{``0})">
            <summary>
            Find the maximum item in this RDD.
            
            sc.Parallelize(new double[] { 1.0, 5.0, 43.0, 10.0 }, 2).Max()
            43.0
            
            </summary>
            <typeparam name="T"></typeparam>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.ComparableRDDFunctions.Min``1(Microsoft.Spark.CSharp.Core.RDD{``0})">
            <summary>
            Find the minimum item in this RDD.
            
            sc.Parallelize(new double[] { 2.0, 5.0, 43.0, 10.0 }, 2).Min()
            >>> rdd.min()
            2.0
            
            </summary>
            <typeparam name="T"></typeparam>
            <param name="self"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.ComparableRDDFunctions.TakeOrdered``1(Microsoft.Spark.CSharp.Core.RDD{``0},System.Int32)">
            <summary>
            Get the N elements from a RDD ordered in ascending order or as
            specified by the optional key function.
            
            sc.Parallelize(new int[] { 10, 1, 2, 9, 3, 4, 5, 6, 7 }, 2).TakeOrdered(6)
            [1, 2, 3, 4, 5, 6]
            
            </summary>
            <typeparam name="T"></typeparam>
            <param name="self"></param>
            <param name="num"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.ComparableRDDFunctions.Top``1(Microsoft.Spark.CSharp.Core.RDD{``0},System.Int32)">
            <summary>
            Get the top N elements from a RDD.
            
            Note: It returns the list sorted in descending order.
            
            sc.Parallelize(new int[] { 2, 3, 4, 5, 6 }, 2).Top(3)
            [6, 5, 4]
            
            </summary>
            <typeparam name="T"></typeparam>
            <param name="self"></param>
            <param name="num"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.DynamicTypingWrapper`2">
            <summary>
            This class is used to wrap Func of specific parameter types into Func of dynamic parameter types.  The wrapping is done to use dynamic types 
            in PipelinedRDD which helps keeping the deserialization of Func simple at the worker side.
            
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types 
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.DynamicTypingWrapper`5">
            <summary>
            This class is used to wrap Func of specific parameter types into Func of dynamic parameter types.  The wrapping is done to use dynamic types 
            in PipelinedRDD which helps keeping the deserialization of Func simple at the worker side.
            
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types 
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.FilterHelper`1">
            <summary>
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.MapHelper`2">
            <summary>
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.FlatMapHelper`2">
            <summary>
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.MapPartitionsHelper`2">
            <summary>
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.ReduceHelper`1">
            <summary>
            This class is defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. This class is to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Core.SparkConf">
             <summary>
             Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.
            
             Note that once a SparkConf object is passed to Spark, it is cloned and can no longer be modified
             by the user. Spark does not support modifying the configuration at runtime.
             
             See also http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf
             </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.#ctor(Microsoft.Spark.CSharp.Proxy.ISparkConfProxy)">
            <summary>
            when created from checkpoint
            </summary>
            <param name="sparkConfProxy"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.#ctor(System.Boolean)">
            <summary>
            Create SparkConf
            </summary>
            <param name="loadDefaults">indicates whether to also load values from Java system properties</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.SetMaster(System.String)">
            <summary>
            The master URL to connect to, such as "local" to run locally with one thread, "local[4]" to 
            run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster.
            </summary>
            <param name="master">Spark master</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.SetAppName(System.String)">
            <summary>
            Set a name for your application. Shown in the Spark web UI.
            </summary>
            <param name="appName">Name of the app</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.SetSparkHome(System.String)">
            <summary>
            Set the location where Spark is installed on worker nodes.
            </summary>
            <param name="sparkHome"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.Set(System.String,System.String)">
            <summary>
            Set the value of a string config
            </summary>
            <param name="key">Config name</param>
            <param name="value">Config value</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.GetInt(System.String,System.Int32)">
            <summary>
            Get a int parameter value, falling back to a default if not set
            </summary>
            <param name="key">Key to use</param>
            <param name="defaultValue">Default value to use</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkConf.Get(System.String,System.String)">
            <summary>
            Get a string parameter value, falling back to a default if not set
            </summary>
            <param name="key">Key to use</param>
            <param name="defaultValue">Default value to use</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.#ctor(Microsoft.Spark.CSharp.Proxy.ISparkContextProxy,Microsoft.Spark.CSharp.Core.SparkConf)">
            <summary>
            when created from checkpoint
            </summary>
            <param name="sparkContextProxy"></param>
            <param name="conf"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.Parallelize``1(System.Collections.Generic.IEnumerable{``0},System.Int32)">
             <summary>
             Distribute a local collection to form an RDD.
            
             sc.Parallelize(new int[] {0, 2, 3, 4, 6}, 5).Glom().Collect()
             [[0], [2], [3], [4], [6]]
             
             </summary>
             <typeparam name="T"></typeparam>
             <param name="serializableObjects"></param>
             <param name="numSlices"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.EmptyRDD``1">
            <summary>
            Create an RDD that has no partitions or elements.
            </summary>
            <typeparam name="T"></typeparam>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.WholeTextFiles(System.String,System.Nullable{System.Int32})">
             <summary>
             Read a directory of text files from HDFS, a local file system (available on all nodes), or any
             Hadoop-supported file system URI. Each file is read as a single record and returned in a
             key-value pair, where the key is the path of each file, the value is the content of each file.
            
             For example, if you have the following files:
             {{{
               hdfs://a-hdfs-path/part-00000
               hdfs://a-hdfs-path/part-00001
               ...
               hdfs://a-hdfs-path/part-nnnnn
             }}}
            
             Do
             {{{
               <see cref="!:RDD&lt;KeyValuePair&lt;string, string&gt;&gt;"/> rdd = sparkContext.WholeTextFiles("hdfs://a-hdfs-path")
             }}}
            
             then `rdd` contains
             {{{
               (a-hdfs-path/part-00000, its content)
               (a-hdfs-path/part-00001, its content)
               ...
               (a-hdfs-path/part-nnnnn, its content)
             }}}
            
             Small files are preferred, large file is also allowable, but may cause bad performance.
            
             minPartitions A suggestion value of the minimal splitting number for input data.
             </summary>
             <param name="filePath"></param>
             <param name="minPartitions"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.BinaryFiles(System.String,System.Nullable{System.Int32})">
             <summary>
             Read a directory of binary files from HDFS, a local file system (available on all nodes),
             or any Hadoop-supported file system URI as a byte array. Each file is read as a single
             record and returned in a key-value pair, where the key is the path of each file,
             the value is the content of each file.
            
             For example, if you have the following files:
             {{{
               hdfs://a-hdfs-path/part-00000
               hdfs://a-hdfs-path/part-00001
               ...
               hdfs://a-hdfs-path/part-nnnnn
             }}}
            
             Do
             <see cref="!:RDD&lt;KeyValuePair&lt;string, byte[]&gt;&gt;"/> rdd = sparkContext.dataStreamFiles("hdfs://a-hdfs-path")`,
            
             then `rdd` contains
             {{{
               (a-hdfs-path/part-00000, its content)
               (a-hdfs-path/part-00001, its content)
               ...
               (a-hdfs-path/part-nnnnn, its content)
             }}}
            
             @note Small files are preferred; very large files but may cause bad performance.
            
             @param minPartitions A suggestion value of the minimal splitting number for input data.
             </summary>
             <param name="filePath"></param>
             <param name="minPartitions"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.SequenceFile(System.String,System.String,System.String,System.String,System.String,System.Nullable{System.Int32})">
            <summary>
            Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,
            a local file system (available on all nodes), or any Hadoop-supported file system URI.
            The mechanism is as follows:
            
                1. A Java RDD is created from the SequenceFile or other InputFormat, and the key
                    and value Writable classes
                2. Serialization is attempted via Pyrolite pickling
                3. If this fails, the fallback is to call 'toString' on each key and value
                4. PickleSerializer is used to deserialize pickled objects on the Python side
                
            </summary>
            <param name="filePath">path to sequncefile</param>
            <param name="keyClass">fully qualified classname of key Writable class (e.g. "org.apache.hadoop.io.Text")</param>
            <param name="valueClass">fully qualified classname of value Writable class (e.g. "org.apache.hadoop.io.LongWritable")</param>
            <param name="keyConverterClass"></param>
            <param name="valueConverterClass"></param>
            <param name="minSplits">minimum splits in dataset (default min(2, sc.defaultParallelism))</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.NewAPIHadoopFile(System.String,System.String,System.String,System.String,System.String,System.String,System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}})">
            <summary>
            Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,
            a local file system (available on all nodes), or any Hadoop-supported file system URI.
            The mechanism is the same as for sc.sequenceFile.
            
            A Hadoop configuration can be passed in as a Python dict. This will be converted into a Configuration in Java
            
            </summary>
            <param name="filePath">path to Hadoop file</param>
            <param name="inputFormatClass">fully qualified classname of Hadoop InputFormat (e.g. "org.apache.hadoop.mapreduce.lib.input.TextInputFormat")</param>
            <param name="keyClass">fully qualified classname of key Writable class (e.g. "org.apache.hadoop.io.Text")</param>
            <param name="valueClass">fully qualified classname of value Writable class (e.g. "org.apache.hadoop.io.LongWritable")</param>
            <param name="keyConverterClass">(None by default)</param>
            <param name="valueConverterClass">(None by default)</param>
            <param name="conf"> Hadoop configuration, passed in as a dict (None by default)</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.NewAPIHadoopRDD(System.String,System.String,System.String,System.String,System.String,System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}})">
            <summary>
            Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary
            Hadoop configuration, which is passed in as a Python dict.
            This will be converted into a Configuration in Java.
            The mechanism is the same as for sc.sequenceFile.
            
            </summary>
            <param name="inputFormatClass">fully qualified classname of Hadoop InputFormat (e.g. "org.apache.hadoop.mapreduce.lib.input.TextInputFormat")</param>
            <param name="keyClass">fully qualified classname of key Writable class (e.g. "org.apache.hadoop.io.Text")</param>
            <param name="valueClass">fully qualified classname of value Writable class (e.g. "org.apache.hadoop.io.LongWritable")</param>
            <param name="keyConverterClass">(None by default)</param>
            <param name="valueConverterClass">(None by default)</param>
            <param name="conf">Hadoop configuration, passed in as a dict (None by default)</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.HadoopFile(System.String,System.String,System.String,System.String,System.String,System.String,System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}})">
            <summary>
            Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,
            a local file system (available on all nodes), or any Hadoop-supported file system URI.
            The mechanism is the same as for sc.sequenceFile.
            
            A Hadoop configuration can be passed in as a Python dict. This will be converted into a Configuration in Java.
            
            </summary>
            <param name="filePath">path to Hadoop file</param>
            <param name="inputFormatClass">fully qualified classname of Hadoop InputFormat (e.g. "org.apache.hadoop.mapred.TextInputFormat")</param>
            <param name="keyClass">fully qualified classname of key Writable class (e.g. "org.apache.hadoop.io.Text")</param>
            <param name="valueClass">fully qualified classname of value Writable class (e.g. "org.apache.hadoop.io.LongWritable")</param>
            <param name="keyConverterClass">(None by default)</param>
            <param name="valueConverterClass">(None by default)</param>
            <param name="conf">Hadoop configuration, passed in as a dict (None by default)</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.HadoopRDD(System.String,System.String,System.String,System.String,System.String,System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{System.String,System.String}})">
            <summary>
            Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary
            Hadoop configuration, which is passed in as a Python dict.
            This will be converted into a Configuration in Java.
            The mechanism is the same as for sc.sequenceFile.
            
            </summary>
            <param name="inputFormatClass">fully qualified classname of Hadoop InputFormat (e.g. "org.apache.hadoop.mapred.TextInputFormat")</param>
            <param name="keyClass">fully qualified classname of key Writable class (e.g. "org.apache.hadoop.io.Text")</param>
            <param name="valueClass">fully qualified classname of value Writable class (e.g. "org.apache.hadoop.io.LongWritable")</param>
            <param name="keyConverterClass">(None by default)</param>
            <param name="valueConverterClass">(None by default)</param>
            <param name="conf">Hadoop configuration, passed in as a dict (None by default)</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.Union``1(System.Collections.Generic.IEnumerable{Microsoft.Spark.CSharp.Core.RDD{``0}})">
            <summary>
            Build the union of a list of RDDs.
            
            This supports unions() of RDDs with different serialized formats,
            although this forces them to be reserialized using the default serializer:
            
            >>> path = os.path.join(tempdir, "union-text.txt")
            >>> with open(path, "w") as testFile:
            ...    _ = testFile.write("Hello")
            >>> textFile = sc.textFile(path)
            >>> textFile.collect()
            [u'Hello']
            >>> parallelized = sc.parallelize(["World!"])
            >>> sorted(sc.union([textFile, parallelized]).collect())
            [u'Hello', 'World!']
            </summary>
            <typeparam name="T"></typeparam>
            <param name="rdds"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.Broadcast``1(``0)">
            <summary>
            Broadcast a read-only variable to the cluster, returning a Broadcast
            object for reading it in distributed functions. The variable will
            be sent to each cluster only once.
            </summary>
            <typeparam name="T"></typeparam>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.Accumulator``1(``0)">
            <summary>
            Create an <see cref="M:Microsoft.Spark.CSharp.Core.SparkContext.Accumulator``1(``0)"/> with the given initial value, using a given
            <see cref="T:Microsoft.Spark.CSharp.Core.AccumulatorParam`1"/> helper object to define how to add values of the
            data type if provided. Default AccumulatorParams are used for integers
            and floating-point numbers if you do not provide one. For other types,
            a custom AccumulatorParam can be used.
            </summary>
            <typeparam name="T"></typeparam>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.Stop">
            <summary>
            Shut down the SparkContext.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.AddFile(System.String)">
            <summary>
            Add a file to be downloaded with this Spark job on every node.
            The `path` passed can be either a local file, a file in HDFS (or other Hadoop-supported
            filesystems), or an HTTP, HTTPS or FTP URI.  To access the file in Spark jobs,
            use `SparkFiles.get(fileName)` to find its download location.
            </summary>
            <param name="path"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.SetCheckpointDir(System.String)">
            <summary>
            Set the directory under which RDDs are going to be checkpointed. The directory must
            be a HDFS path if running on a cluster.
            </summary>
            <param name="directory"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.SetJobGroup(System.String,System.String,System.Boolean)">
             <summary>
             Assigns a group ID to all the jobs started by this thread until the group ID is set to a
             different value or cleared.
            
             Often, a unit of execution in an application consists of multiple Spark actions or jobs.
             Application programmers can use this method to group all those jobs together and give a
             group description. Once set, the Spark web UI will associate such jobs with this group.
            
             The application can also use [[org.apache.spark.api.java.JavaSparkContext.cancelJobGroup]]
             to cancel all running jobs in this group. For example,
             {{{
             // In the main thread:
             sc.setJobGroup("some_job_to_cancel", "some job description");
             rdd.map(...).count();
            
             // In a separate thread:
             sc.cancelJobGroup("some_job_to_cancel");
             }}}
            
             If interruptOnCancel is set to true for the job group, then job cancellation will result
             in Thread.interrupt() being called on the job's executor threads. This is useful to help ensure
             that the tasks are actually stopped in a timely manner, but is off by default due to HDFS-1208,
             where HDFS may respond to Thread.interrupt() by marking nodes as dead.
             </summary>
             <param name="groupId"></param>
             <param name="description"></param>
             <param name="interruptOnCancel"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.SetLocalProperty(System.String,System.String)">
            <summary>
            Set a local property that affects jobs submitted from this thread, such as the
            Spark fair scheduler pool.
            </summary>
            <param name="key"></param>
            <param name="value"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.GetLocalProperty(System.String)">
            <summary>
            Get a local property set in this thread, or null if it is missing. See
            [[org.apache.spark.api.java.JavaSparkContext.setLocalProperty]].
            </summary>
            <param name="key"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.SetLogLevel(System.String)">
            <summary>
            Control our logLevel. This overrides any user-defined log settings.
            @param logLevel The desired log level as a string.
            Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
            </summary>
            <param name="logLevel"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.CancelJobGroup(System.String)">
            <summary>
            Cancel active jobs for the specified group. See <see cref="M:Microsoft.Spark.CSharp.Core.SparkContext.SetJobGroup(System.String,System.String,System.Boolean)"/> for more information.
            </summary>
            <param name="groupId"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.SparkContext.CancelAllJobs">
            <summary>
            Cancel all jobs that have been scheduled or are running.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.SparkContext.Version">
            <summary>
            The version of Spark on which this application is running.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.SparkContext.StartTime">
            <summary>
            Return the epoch time when the Spark Context was started.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.SparkContext.DefaultParallelism">
            <summary>
            Default level of parallelism to use when not given by user (e.g. for reduce tasks)
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.SparkContext.DefaultMinPartitions">
            <summary>
            Default min number of partitions for Hadoop RDDs when not given by user
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.SparkContext.SparkUser">
            <summary>
            Get SPARK_USER for user who is running SparkContext.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.SparkContext.StatusTracker">
            <summary>
            Return :class:`StatusTracker` object
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatCounter.Merge(System.Double)">
            <summary>
            Add a value into this StatCounter, updating the internal statistics.
            </summary>
            <param name="value"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatCounter.Merge(System.Collections.Generic.IEnumerable{System.Double})">
            <summary>
            Add multiple values into this StatCounter, updating the internal statistics.
            </summary>
            <param name="values"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatCounter.Merge(Microsoft.Spark.CSharp.Core.StatCounter)">
            <summary>
            Merge another StatCounter into this one, adding up the internal statistics.
            </summary>
            <param name="other"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatCounter.copy">
            <summary>
            Clone this StatCounter
            </summary>
            <returns></returns>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.StatCounter.Variance">
            <summary>
            Return the variance of the values.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.StatCounter.SampleVariance">
            <summary>
            Return the sample variance, which corrects for bias in estimating the variance by dividing by N-1 instead of N.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.StatCounter.Stdev">
            <summary>
            Return the standard deviation of the values.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Core.StatCounter.SampleStdev">
            <summary>
            Return the sample standard deviation of the values, which corrects for bias in estimating the variance by dividing by N-1 instead of N.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatusTracker.GetJobIdsForGroup(System.String)">
             <summary>
             Return a list of all known jobs in a particular job group.  If
             `jobGroup` is None, then returns all known jobs that are not
             associated with a job group.
            
             The returned list may contain running, failed, and completed jobs,
             and may vary across invocations of this method. This method does
             not guarantee the order of the elements in its result.
             </summary>
             <param name="jobGroup"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatusTracker.GetActiveStageIds">
            <summary>
            Returns an array containing the ids of all active stages.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatusTracker.GetActiveJobsIds">
            <summary>
            Returns an array containing the ids of all active jobs.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatusTracker.GetJobInfo(System.Int32)">
            <summary>
            Returns a :class:`SparkJobInfo` object, or None if the job info
            could not be found or was garbage collected.
            </summary>
            <param name="jobId"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Core.StatusTracker.GetStageInfo(System.Int32)">
            <summary>
            Returns a :class:`SparkStageInfo` object, or None if the stage
            info could not be found or was garbage collected.
            </summary>
            <param name="stageId"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Interop.Ipc.JsonSerDe">
            <summary>
            Json.NET Serialization/Deserialization helper class.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Interop.Ipc.JsonSerDe.SortProperties(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Extend method to sort items in a JSON object by keys.
            </summary>
            <param name="jObject"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Interop.Ipc.JsonSerDe.SortProperties(Newtonsoft.Json.Linq.JArray)">
            <summary>
            Extend method to sort items in a JSON array by keys.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Interop.Ipc.JvmBridgeUtils">
            <summary>
            Utility methods for C#-JVM interaction
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Interop.SparkCLREnvironment">
            <summary>
            Contains everything needed to setup an environment for using C# with Spark
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.Spark.CSharp.Interop.Ipc.IJvmBridge" -->
        <!-- Badly formed XML comment ignored for member "T:Microsoft.Spark.CSharp.Interop.Ipc.JvmBridge" -->
        <member name="T:Microsoft.Spark.CSharp.Interop.Ipc.JvmObjectReference">
            <summary>
            Reference to object created in JVM
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Interop.Ipc.PayloadHelper">
            <summary>
            Help build the IPC payload for JVM calls from CLR
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Interop.Ipc.SpecialLengths">
            <summary>
            see PythonRDD.scala with which Work.cs communicates
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.Spark.CSharp.Interop.Ipc.SerDe" -->
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Intersect(Microsoft.Spark.CSharp.Proxy.IDataFrameProxy)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, intersect(other: DataFrame): DataFrame
            </summary>
            <param name="otherScalaDataFrameReference"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.UnionAll(Microsoft.Spark.CSharp.Proxy.IDataFrameProxy)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, unionAll(other: DataFrame): DataFrame
            </summary>
            <param name="otherScalaDataFrameReference"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Subtract(Microsoft.Spark.CSharp.Proxy.IDataFrameProxy)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, except(other: DataFrame): DataFrame
            </summary>
            <param name="otherScalaDataFrameReference"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Drop(System.String)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, drop(colName: String): DataFrame
            </summary>
            <param name="columnName"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Na">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, na(): DataFrame
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.DropDuplicates">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, dropDuplicates(): DataFrame
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.DropDuplicates(System.String[])">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, dropDuplicates(colNames: Seq[String]): DataFrame
            </summary>
            <param name="subset"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Replace``1(System.Object,System.Collections.Generic.Dictionary{``0,``0})">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala, replace[T](col: String, replacement: Map[T, T]): DataFrame
            </summary>
            <typeparam name="T"></typeparam>
            <param name="subset"></param>
            <param name="toReplaceAndValueDict"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.RandomSplit(System.Collections.Generic.IEnumerable{System.Double},System.Nullable{System.Int64})">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, randomSplit(weights: Array[Double], seed: Long): Array[DataFrame]
            </summary>
            <param name="weights"></param>
            <param name="seed"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Sort(Microsoft.Spark.CSharp.Proxy.IColumnProxy[])">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, sort(sortExprs: Column*): DataFrame
            </summary>
            <param name="columns"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.SortWithinPartitions(Microsoft.Spark.CSharp.Proxy.IColumnProxy[])">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, sortWithinPartitions(sortExprs: Column*): DataFrame
            </summary>
            <param name="columns"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Alias(System.String)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, as(alias: String): DataFrame
            </summary>
            <param name="alias"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Corr(System.String,System.String,System.String)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, corr(col1: String, col2: String, method: String): Double
            </summary>
            <param name="column1"></param>
            <param name="column2"></param>
            <param name="method"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Cov(System.String,System.String)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, cov(col1: String, col2: String): Double
            </summary>
            <param name="column1"></param>
            <param name="column2"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.FreqItems(System.Collections.Generic.IEnumerable{System.String},System.Double)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, freqItems(cols: Array[String], support: Double)
            </summary>
            <param name="columns"></param>
            <param name="support"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Crosstab(System.String,System.String)">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, crosstab(col1: String, col2: String): DataFrame
            </summary>
            <param name="column1"></param>
            <param name="column2"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Repartition(System.Int32,Microsoft.Spark.CSharp.Proxy.IColumnProxy[])">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, repartition(numPartitions: Int, partitionExprs: Column*): DataFrame
            </summary>
            <param name="numPartitions"></param>
            <param name="columns"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.DataFrameIpcProxy.Repartition(Microsoft.Spark.CSharp.Proxy.IColumnProxy[])">
            <summary>
            Call https://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala, repartition(partitionExprs: Column*): DataFrame
            </summary>
            <param name="columns"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Proxy.Ipc.DStreamIpcProxy">
            <summary>
            calling Spark jvm side API in JavaDStream.scala, DStream.scala or CSharpDStream.scala
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Proxy.Ipc.SparkCLRIpcProxy">
            <summary>
            calling SparkCLR jvm side API
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Proxy.Ipc.StreamingContextIpcProxy">
            <summary>
            calling Spark jvm side API in JavaStreamingContext.scala, StreamingContext.scala or external KafkaUtils.scala
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Proxy.Ipc.SparkContextIpcProxy.CreatePairwiseRDD(Microsoft.Spark.CSharp.Proxy.IRDDProxy,System.Int32,System.Int64)">
            <summary>
            Create a PairwiseRDD.
            </summary>
            <param name="jvmReferenceOfByteArrayRdd"></param>
            <param name="numPartitions"></param>
            <param name="partitionFuncId">Global unique id of partitioner which is used for comparison PythonPartitioners in JVM.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Services.DefaultLoggerService">
            <summary>
            This logger service will be used if the C# driver app did not configure a logger.
            Right now it just prints out the messages to Console
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Services.LoggerServiceFactory">
            <summary>
            Used to get logger service instances for different types
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Column.GetHashCode">
            <summary>
            Required when operator == or operator != is defined
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Column.Equals(System.Object)">
            <summary>
            Required when operator == or operator != is defined
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Column.Like(System.String)">
            <summary>
            SQL like expression.
            </summary>
            <param name="literal"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Column.RLike(System.String)">
            <summary>
            SQL RLIKE expression (LIKE with Regex).
            </summary>
            <param name="literal"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Column.StartsWith(Microsoft.Spark.CSharp.Sql.Column)">
            <summary>
            String starts with another string literal.
            </summary>
            <param name="other"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Column.EndsWith(Microsoft.Spark.CSharp.Sql.Column)">
            <summary>
            String ends with another string literal.
            </summary>
            <param name="other"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Column.Cast(System.String)">
            <summary>
            Casts the column to a different data type, using the canonical string representation
            of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
            `float`, `double`, `decimal`, `date`, `timestamp`.
            
            E.g.
                // Casts colA to integer.
                df.select(df("colA").cast("int"))
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.DataFrame">
            <summary>
             A distributed collection of data organized into named columns.
            
            See also http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.RegisterTempTable(System.String)">
            <summary>
            Registers this DataFrame as a temporary table using the given name.  The lifetime of this 
            temporary table is tied to the SqlContext that was used to create this DataFrame.
            </summary>
            <param name="tableName">Name of the table</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Count">
            <summary>
            Number of rows in the DataFrame
            </summary>
            <returns>row count</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Show(System.Int32,System.Boolean)">
            <summary>
            Displays rows of the DataFrame in tabular form
            </summary>
            <param name="numberOfRows">Number of rows to display - default 20</param>
            <param name="truncate">Indicates if strings more than 20 characters long will be truncated</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.ShowSchema">
            <summary>
            Prints the schema information of the DataFrame
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Collect">
            <summary>
            Returns all of Rows in this DataFrame
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.ToRDD">
            <summary>
            Converts the DataFrame to RDD of Row
            </summary>
            <returns>resulting RDD</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.ToJSON">
            <summary>
            Returns the content of the DataFrame as RDD of JSON strings
            </summary>
            <returns>resulting RDD</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Explain(System.Boolean)">
            <summary>
            Prints the plans (logical and physical) to the console for debugging purposes
            </summary>
            <param name="extended">if true prints both query plan and execution plan; otherwise just prints query plan</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Select(System.Object,System.Object[])">
            <summary>
            Selects a set of columns specified by column name or Column.
            
            df.Select("colA", df["colB"])
            df.Select("*", df["colB"] + 10)
            
            </summary>
            <param name="firstColumn">first column - required, must be of type string or Column</param>
            <param name="otherColumns">other column - optional, must be of type string or Column</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Select(System.String,System.String[])">
            <summary>
            Selects a set of columns. This is a variant of `select` that can only select
            existing columns using column names (i.e. cannot construct expressions).
            
            df.Select("colA", "colB")
            
            </summary>
            <param name="firstColumnName">first column name - required</param>
            <param name="otherColumnNames">other column names - optional</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.SelectExpr(System.String[])">
             <summary>
             Selects a set of SQL expressions. This is a variant of `select` that accepts SQL expressions.
            
               df.SelectExpr("colA", "colB as newName", "abs(colC)")
               
             </summary>
             <param name="columnExpressions"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Where(System.String)">
            <summary>
            Filters rows using the given condition
            </summary>
            <param name="condition"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Filter(System.String)">
            <summary>
            Filters rows using the given condition
            </summary>
            <param name="condition"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.GroupBy(System.String,System.String[])">
            <summary>
            Groups the DataFrame using the specified columns, so we can run aggregation on them.
            </summary>
            <param name="firstColumnName">first column name - required</param>
            <param name="otherColumnNames">other column names - optional</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Rollup(System.String,System.String[])">
            <summary>
            Create a multi-dimensional rollup for the current DataFrame using the specified columns, so we can run aggregation on them.
            </summary>
            <param name="firstColumnName">first column name - required</param>
            <param name="otherColumnNames">other column names - optional</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Cube(System.String,System.String[])">
            <summary>
            Create a multi-dimensional cube for the current DataFrame using the specified columns, so we can run aggregation on them.
            </summary>
            <param name="firstColumnName">first column name - required</param>
            <param name="otherColumnNames">other column names - optional</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Agg(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Aggregates on the DataFrame for the given column-aggregate function mapping
            </summary>
            <param name="columnNameAggFunctionDictionary"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Join(Microsoft.Spark.CSharp.Sql.DataFrame)">
            <summary>
            Join with another DataFrame - Cartesian join
            </summary>
            <param name="otherDataFrame">DataFrame to join with</param>
            <returns>Joined DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Join(Microsoft.Spark.CSharp.Sql.DataFrame,System.String)">
            <summary>
            Join with another DataFrame - Inner equi-join using given column name
            </summary>
            <param name="otherDataFrame">DataFrame to join with</param>
            <param name="joinColumnName">Column to join with.</param>
            <returns>Joined DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Join(Microsoft.Spark.CSharp.Sql.DataFrame,System.String[])">
            <summary>
            Join with another DataFrame - Inner equi-join using given column name 
            </summary>
            <param name="otherDataFrame">DataFrame to join with</param>
            <param name="joinColumnNames">Columns to join with.</param>
            <returns>Joined DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Join(Microsoft.Spark.CSharp.Sql.DataFrame,Microsoft.Spark.CSharp.Sql.Column,Microsoft.Spark.CSharp.Sql.JoinType)">
            <summary>
            Join with another DataFrame, using the specified JoinType
            </summary>
            <param name="otherDataFrame">DataFrame to join with</param>
            <param name="joinExpression">Column to join with.</param>
            <param name="joinType">Type of join to perform (default null value means <c>JoinType.Inner</c>)</param>
            <returns>Joined DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Intersect(Microsoft.Spark.CSharp.Sql.DataFrame)">
            <summary>
            Intersect with another DataFrame.
            This is equivalent to `INTERSECT` in SQL.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, intersect(self, other)
            </summary>
            <param name="otherDataFrame">DataFrame to intersect with.</param>
            <returns>Intersected DataFrame.</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.UnionAll(Microsoft.Spark.CSharp.Sql.DataFrame)">
            <summary>
            Union with another DataFrame WITHOUT removing duplicated rows.
            This is equivalent to `UNION ALL` in SQL.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, unionAll(self, other)
            </summary>
            <param name="otherDataFrame">DataFrame to union all with.</param>
            <returns>Unioned DataFrame.</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Subtract(Microsoft.Spark.CSharp.Sql.DataFrame)">
            <summary>
            Returns a new DataFrame containing rows in this frame but not in another frame.
            This is equivalent to `EXCEPT` in SQL.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, subtract(self, other)
            </summary>
            <param name="otherDataFrame">DataFrame to subtract from this frame.</param>
            <returns>A new DataFrame containing rows in this frame but not in another frame.</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Drop(System.String)">
            <summary>
            Returns a new DataFrame with a column dropped.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, drop(self, col)
            </summary>
            <param name="columnName"> a string name of the column to drop</param>
            <returns>A new new DataFrame that drops the specified column.</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.DropNa(System.String,System.Nullable{System.Int32},System.String[])">
            <summary>
            Returns a new DataFrame omitting rows with null values.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, dropna(self, how='any', thresh=None, subset=None)
            </summary>
            <param name="how">'any' or 'all'. 
            If 'any', drop a row if it contains any nulls.
            If 'all', drop a row only if all its values are null.</param>
            <param name="thresh">thresh: int, default null.
            If specified, drop rows that have less than `thresh` non-null values.
            This overwrites the `how` parameter.</param>
            <param name="subset">optional list of column names to consider.</param>
            <returns>A new DataFrame omitting rows with null values</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Na">
            <summary>
            Returns a DataFrameNaFunctions for working with missing data.
            </summary>
            Reference: https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, na(self)
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.FillNa(System.Object,System.String[])">
            <summary>
            Replace null values, alias for ``na.fill()`
            </summary>
            <param name="value">
            Value to replace null values with.
            Value type should be float, double, short, int, long, string or Dictionary.
            If the value is a dict, then `subset` is ignored and `value` must be a mapping
            from column name (string) to replacement value. The replacement value must be
            an int, long, float, or string.
            </param>
            <param name="subset">
            optional list of column names to consider.
            Columns specified in subset that do not have matching data type are ignored.
            For example, if `value` is a string, and subset contains a non-string column,
            then the non-string column is simply ignored.
            </param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.DropDuplicates(System.String[])">
            <summary>
            Returns a new DataFrame with duplicate rows removed, considering only the subset of columns.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, dropDuplicates(self, subset=None)
            </summary>
            <param name="subset">drop duplicated rows on these columns.</param>
            <returns>A new DataFrame with duplicate rows removed.</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Replace``1(``0,``0,System.String[])">
            <summary>
            Returns a new DataFrame replacing a value with another value.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, replace(self, to_replace, value, subset=None)
            </summary>
            <typeparam name="T">Data type of value to replace.</typeparam>
            <param name="toReplace">Value to be replaced. The value to be replaced must be an int, long, float, or string and must be the same type as <paramref name="value"/>.</param>
            <param name="value">Value to use to replace holes. The replacement value must be an int, long, float, or string and must be the same type as <paramref name="toReplace"/>.</param>
            <param name="subset">Optional list of column names to consider.</param>
            <returns>A new DataFrame replacing a value with another value</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.ReplaceAll``1(System.Collections.Generic.IEnumerable{``0},System.Collections.Generic.IEnumerable{``0},System.String[])">
            <summary>
            Returns a new DataFrame replacing values with other values.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, replace(self, to_replace, value, subset=None)
            </summary>
            <typeparam name="T">Data type of values to replace.</typeparam>
            <param name="toReplace">List of values to be replaced. The value to be replaced must be an int, long, float, or string and must be the same type as <paramref name="value"/>. 
            This list should be of the same length with <paramref name="value"/>.</param>
            <param name="value">List of values to replace holes. The replacement must be an int, long, float, or string and must be the same type as <paramref name="toReplace"/>.
            This list should be of the same length with <paramref name="toReplace"/>.</param>
            <param name="subset">Optional list of column names to consider.</param>
            <returns>A new DataFrame replacing values with other values</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.ReplaceAll``1(System.Collections.Generic.IEnumerable{``0},``0,System.String[])">
            <summary>
            Returns a new DataFrame replacing values with another value.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, replace(self, to_replace, value, subset=None)
            </summary>
            <typeparam name="T">Data type of values to replace.</typeparam>
            <param name="toReplace">List of values to be replaced. The value to be replaced must be an int, long, float, or string and must be the same type as <paramref name="value"/>.</param>
            <param name="value">Value to use to replace holes. The replacement value must be an int, long, float, or string and must be the same type as <paramref name="toReplace"/>.</param>
            <param name="subset">Optional list of column names to consider.</param>
            <returns>A new DataFrame replacing values with another value</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.RandomSplit(System.Collections.Generic.IEnumerable{System.Double},System.Nullable{System.Int32})">
            <summary>
            Randomly splits this DataFrame with the provided weights.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, randomSplit(self, weights, seed=None)
            </summary>
            <param name="weights">list of weights with which to split the DataFrame. Weights will be normalized if they don't sum up to 1.0</param>
            <param name="seed">The seed for sampling</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Columns">
            <summary>
            Returns all column names as a list.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, columns(self)
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.DTypes">
            <summary>
            Returns all column names and their data types.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, dtypes(self)
            </summary>
            <returns>Column names and their data types.</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Sort(System.String[],System.Boolean[])">
            <summary>
            Returns a new DataFrame sorted by the specified column(s).
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, sort(self, *cols, **kwargs)
            </summary>
            <param name="columns">List of column names to sort by</param>
            <param name="ascending">List of boolean to specify multiple sort orders for <paramref name="columns"/>, TRUE for ascending, FALSE for descending</param>
            <returns>A new DataFrame sorted by the specified column(s)</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Sort(Microsoft.Spark.CSharp.Sql.Column[],System.Boolean[])">
            <summary>
            Returns a new DataFrame sorted by the specified column(s).
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, sort(self, *cols, **kwargs)
            </summary>
            <param name="columns">List of Columns to sort by</param>
            <param name="ascending">List of boolean to specify multiple sort orders for <paramref name="columns"/>, TRUE for ascending, FALSE for descending.
            if not null, it will overwrite the order specified by Column.Asc() or Column Desc() in <paramref name="columns"/>, </param>
            <returns>A new DataFrame sorted by the specified column(s)</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.SortWithinPartitions(System.String[],System.Boolean[])">
            <summary>
            Returns a new DataFrame sorted by the specified column(s).
            Reference to https://github.com/apache/spark/blob/branch-1.6/python/pyspark/sql/dataframe.py, sortWithinPartitions(self, *cols, **kwargs)
            </summary>
            <param name="columns">List of Columns to sort by</param>
            <param name="ascending">List of boolean to specify multiple sort orders for <paramref name="columns"/>, TRUE for ascending, FALSE for descending.
            if not null, it will overwrite the order specified by Column.Asc() or Column Desc() in <paramref name="columns"/>, </param>
            <returns>A new DataFrame sorted by the specified column(s)</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.SortWithinPartition(Microsoft.Spark.CSharp.Sql.Column[],System.Boolean[])">
            <summary>
            Returns a new DataFrame sorted by the specified column(s).
            Reference to https://github.com/apache/spark/blob/branch-1.6/python/pyspark/sql/dataframe.py, sortWithinPartitions(self, *cols, **kwargs)
            </summary>
            <param name="columns">List of Columns to sort by</param>
            <param name="ascending">List of boolean to specify multiple sort orders for <paramref name="columns"/>, TRUE for ascending, FALSE for descending.
            if not null, it will overwrite the order specified by Column.Asc() or Column Desc() in <paramref name="columns"/>, </param>
            <returns>A new DataFrame sorted by the specified column(s)</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Alias(System.String)">
            <summary>
            Returns a new DataFrame with an alias set.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, alias(self, alias) 
            </summary>
            <param name="alias">The alias of the DataFrame</param>
            <returns>A new DataFrame with an alias set</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.WithColumn(System.String,Microsoft.Spark.CSharp.Sql.Column)">
            <summary>
            Returns a new DataFrame by adding a column.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, withColumn(self, colName, col)
            </summary>
            <param name="newColName">name of the new column</param>
            <param name="column">a Column expression for the new column</param>
            <returns>A new DataFrame with the added column</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.WithColumnRenamed(System.String,System.String)">
            <summary>
            Returns a new DataFrame by renaming an existing column.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, withColumnRenamed(self, existing, new)
            </summary>
            <param name="existingName">name of an existing column</param>
            <param name="newName">new name</param>
            <returns>A new DataFrame with renamed column</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Corr(System.String,System.String,System.String)">
            <summary>
            Calculates the correlation of two columns of a DataFrame as a double value.
            Currently only supports the Pearson Correlation Coefficient.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, corr(self, col1, col2, method=None)
            </summary>
            <param name="column1">The name of the first column</param>
            <param name="column2">The name of the second column</param>
            <param name="method">The correlation method. Currently only supports "pearson"</param>
            <returns>The correlation of two columns</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Cov(System.String,System.String)">
            <summary>
            Calculate the sample covariance of two columns as a double value.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, cov(self, col1, col2)
            </summary>
            <param name="column1">The name of the first column</param>
            <param name="column2">The name of the second column</param>
            <returns>The sample covariance of two columns</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.FreqItems(System.Collections.Generic.IEnumerable{System.String},System.Double)">
            <summary>
            Finding frequent items for columns, possibly with false positives. Using the frequent element count algorithm described in 
            "http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou".
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, freqItems(self, cols, support=None)
            
            <alert class="note">Note: This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility 
            of the schema of the resulting DataFrame. </alert>
            </summary>
            <param name="columns"></param>
            <param name="support"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Crosstab(System.String,System.String)">
            <summary>
            Computes a pair-wise frequency table of the given columns. Also known as a contingency table.
            The number of distinct values for each column should be less than 1e4. At most 1e6 non-zero pair frequencies will be returned.
            Reference to https://github.com/apache/spark/blob/branch-1.4/python/pyspark/sql/dataframe.py, crosstab(self, col1, col2)
            </summary>
            <param name="column1">The name of the first column. Distinct items will make the first item of each row</param>
            <param name="column2">The name of the second column. Distinct items will make the column names of the DataFrame</param>
            <returns>A pair-wise frequency table of the given columns</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Describe(System.String[])">
            <summary>
            Computes statistics for numeric columns.
            This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical columns.
            </summary>
            <param name="columns">Column names to compute statistics.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Limit(System.Int32)">
            <summary>
            Returns a new DataFrame by taking the first `n` rows.
            The difference between this function and `head` is that `head` returns an array while `limit` returns a new DataFrame.
            </summary>
            <param name="num">Number of rows to take from current DataFrame</param>
            <returns>A new DataFrame containing the first `n` rows</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Head(System.Int32)">
            <summary>
            Returns the first `n` rows.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.First">
            <summary>
            Returns the first row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Take(System.Int32)">
            <summary>
            Returns the first `n` rows in the DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Distinct">
            <summary>
            Returns a new DataFrame that contains only the unique rows from this DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Coalesce(System.Int32)">
            <summary>
            Returns a new DataFrame that has exactly `numPartitions` partitions.
            Similar to coalesce defined on an RDD, this operation results in a narrow dependency, 
            e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of
            the 100 new partitions will claim 10 of the current partitions.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Persist">
            <summary>
            Persist this DataFrame with the default storage level (`MEMORY_AND_DISK`)
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Unpersist(System.Boolean)">
            <summary>
            Mark the DataFrame as non-persistent, and remove all blocks for it from memory and disk.
            </summary>
            <param name="blocking">Whether to block until all blocks are deleted.</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Cache">
            <summary>
            Persist this DataFrame with the default storage level (`MEMORY_AND_DISK`)
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Repartition(System.Int32)">
            <summary>
            Returns a new DataFrame that has exactly `numPartitions` partitions.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Repartition(System.String[],System.Int32)">
            <summary>
            Returns a new [[DataFrame]] partitioned by the given partitioning columns into <paramref name="numPartitions"/>. The resulting DataFrame is hash partitioned.
            <param name="columns"></param>
            <param name="numPartitions">optional. If not specified, keep current partitions.</param>
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Repartition(Microsoft.Spark.CSharp.Sql.Column[],System.Int32)">
            <summary>
            Returns a new [[DataFrame]] partitioned by the given partitioning columns into <paramref name="numPartitions"/>. The resulting DataFrame is hash partitioned.
            <param name="columns"></param>
            <param name="numPartitions">optional. If not specified, keep current partitions.</param>
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Sample(System.Boolean,System.Double,System.Nullable{System.Int64})">
            <summary>
            Returns a new DataFrame by sampling a fraction of rows.
            </summary>
            <param name="withReplacement"> Sample with replacement or not. </param>
            <param name="fraction"> Fraction of rows to generate. </param>
            <param name="seed"> Seed for sampling. If it is not present, a randome long value will be assigned. </param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.FlatMap``1(System.Func{Microsoft.Spark.CSharp.Sql.Row,System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Returns a new RDD by first applying a function to all rows of this DataFrame, and then flattening the results.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Map``1(System.Func{Microsoft.Spark.CSharp.Sql.Row,``0})">
            <summary>
            Returns a new RDD by applying a function to all rows of this DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.MapPartitions``1(System.Func{System.Collections.Generic.IEnumerable{Microsoft.Spark.CSharp.Sql.Row},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Returns a new RDD by applying a function to each partition of this DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.ForeachPartition(System.Action{System.Collections.Generic.IEnumerable{Microsoft.Spark.CSharp.Sql.Row}})">
            <summary>
            Applies a function f to each partition of this DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Foreach(System.Action{Microsoft.Spark.CSharp.Sql.Row})">
            <summary>
            Applies a function f to all rows.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Write">
            <summary>
            Interface for saving the content of the DataFrame out into external storage.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.SaveAsParquetFile(System.String)">
            <summary>
            Saves the contents of this DataFrame as a parquet file, preserving the schema.
            Files that are written out using this method can be read back in as a DataFrame
            using the `parquetFile` function in SQLContext.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.InsertInto(System.String,System.Boolean)">
            <summary>
            Adds the rows from this RDD to the specified table, optionally overwriting the existing data.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.SaveAsTable(System.String,System.String,Microsoft.Spark.CSharp.Sql.SaveMode,System.String[])">
            <summary>
            Creates a table from the the contents of this DataFrame based on a given data source, 
            SaveMode specified by mode, and a set of options.
            
            Note that this currently only works with DataFrames that are created from a HiveContext as
            there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
            an RDD out to a parquet file, and then register that file as a table.  This "table" can then
            be the target of an `insertInto`.
            
            Also note that while this function can persist the table metadata into Hive's metastore,
            the table will NOT be accessible from Hive, until SPARK-7550 is resolved.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrame.Save(System.String,System.String,Microsoft.Spark.CSharp.Sql.SaveMode,System.String[])">
            <summary>
            Saves the contents of this DataFrame based on the given data source, 
            SaveMode specified by mode, and a set of options.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions">
            <summary>
            Functionality for working with missing data in DataFrames.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Drop">
            <summary>
            Returns a new DataFrame that drops rows containing any null values.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Drop(System.String)">
            <summary>
            Returns a new DataFrame that drops rows containing null values.
            
            If `how` is "any", then drop rows containing any null values.
            If `how` is "all", then drop rows only if every column is null for that row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Drop(System.String,System.String[])">
            <summary>
            Returns a new [[DataFrame]] that drops rows containing null values
            in the specified columns.
            
            If `how` is "any", then drop rows containing any null values in the specified columns.
            If `how` is "all", then drop rows only if every specified column is null for that row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Drop(System.String[])">
            <summary>
            Returns a new DataFrame that drops rows containing any null values
            in the specified columns.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Drop(System.Int32)">
            <summary>
            Returns a new DataFrame that drops rows containing less than `minNonNulls` non-null values.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Drop(System.Int32,System.String[])">
            <summary>
            Returns a new DataFrame that drops rows containing less than `minNonNulls` non-null values
            values in the specified columns.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Fill(System.Double)">
            <summary>
            Returns a new DataFrame that replaces null values in numeric columns with `value`.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Fill(System.String)">
            <summary>
            Returns a new DataFrame that replaces null values in string columns with `value`.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Fill(System.Double,System.String[])">
            <summary>
            Returns a new DataFrame that replaces null values in specified numeric columns.
            If a specified column is not a numeric column, it is ignored.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Fill(System.String,System.String[])">
            <summary>
            Returns a new DataFrame that replaces null values in specified string columns.
            If a specified column is not a numeric column, it is ignored.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.Dictionary{System.String,System.Object})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            
            Key and value of `replacement` map must have the same type, and can only be doubles or strings.
            The value must be of the following type: `Integer`, `Long`, `Float`, `Double`, `String`.
            
            For example, the following replaces null values in column "A" with string "unknown", and
            null values in column "B" with numeric value 1.0.
            
              import com.google.common.collect.ImmutableMap;
              df.na.fill(ImmutableMap.of("A", "unknown", "B", 1.0));
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Replace``1(System.String,System.Collections.Generic.Dictionary{``0,``0})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            
            Key and value of `replacement` map must have the same type, and can only be doubles or strings.
            If `col` is "*", then the replacement is applied on all string columns or numeric columns.
            
            Example:
            
              import com.google.common.collect.ImmutableMap;
              // Replaces all occurrences of 1.0 with 2.0 in column "height".
              df.replace("height", ImmutableMap.of(1.0, 2.0));
              
              // Replaces all occurrences of "UNKNOWN" with "unnamed" in column "name".
              df.replace("name", ImmutableMap.of("UNKNOWN", "unnamed"));
              // Replaces all occurrences of "UNKNOWN" with "unnamed" in all string columns.
              df.replace("*", ImmutableMap.of("UNKNOWN", "unnamed"));
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameNaFunctions.Replace``1(System.String[],System.Collections.Generic.Dictionary{``0,``0})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            
            Key and value of `replacement` map must have the same type, and can only be doubles or strings.
            If `col` is "*", then the replacement is applied on all string columns or numeric columns.
            
            Example:
            
              import com.google.common.collect.ImmutableMap;
              // Replaces all occurrences of 1.0 with 2.0 in column "height" and "weight".
              df.replace(new String[] {"height", "weight"}, ImmutableMap.of(1.0, 2.0));
             
              // Replaces all occurrences of "UNKNOWN" with "unnamed" in column "firstname" and "lastname".
              df.replace(new String[] {"firstname", "lastname"}, ImmutableMap.of("UNKNOWN", "unnamed"));
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.DataFrameReader">
            <summary>
            Interface used to load a DataFrame from external storage systems (e.g. file systems,
            key-value stores, etc). Use SQLContext.read() to access this.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Format(System.String)">
            <summary>
            Specifies the input data source format.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Schema(Microsoft.Spark.CSharp.Sql.StructType)">
            <summary>
            Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema
            automatically from data. By specifying the schema here, the underlying data source can
            skip the schema inference step, and thus speed up data loading.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Option(System.String,System.String)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds input options for the underlying data source.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Load(System.String)">
            <summary>
            Loads input in as a [[DataFrame]], for data sources that require a path (e.g. data backed by
            a local or distributed file system).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Load">
            <summary>
            Loads input in as a DataFrame, for data sources that don't require a path (e.g. external
            key-value stores).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Jdbc(System.String,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Construct a [[DataFrame]] representing the database table accessible via JDBC URL,
            url named table and connection properties.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Jdbc(System.String,System.String,System.String,System.String,System.String,System.Int32,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Construct a DataFrame representing the database table accessible via JDBC URL
            url named table. Partitions of the table will be retrieved in parallel based on the parameters
            passed to this function.
            
            Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
            your external database systems.
            </summary>
            <param name="url">JDBC database url of the form `jdbc:subprotocol:subname`</param>
            <param name="table">Name of the table in the external database.</param>
            <param name="columnName">the name of a column of integral type that will be used for partitioning.</param>
            <param name="lowerBound">the minimum value of `columnName` used to decide partition stride</param>
            <param name="upperBound">the maximum value of `columnName` used to decide partition stride</param>
            <param name="numPartitions">the number of partitions.  the range `minValue`-`maxValue` will be split evenly into this many partitions</param>
            <param name="connectionProperties">JDBC database connection arguments, a list of arbitrary string tag/value. 
            Normally at least a "user" and "password" property should be included.</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Jdbc(System.String,System.String,System.String[],System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Construct a DataFrame representing the database table accessible via JDBC URL
            url named table using connection properties. The `predicates` parameter gives a list
            expressions suitable for inclusion in WHERE clauses; each one defines one partition
            of the DataFrame.
            
            Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
            your external database systems.
            </summary>
            <param name="url">JDBC database url of the form `jdbc:subprotocol:subname`</param>
            <param name="table">Name of the table in the external database.</param>
            <param name="predicates">Condition in the where clause for each partition.</param>
            <param name="connectionProperties">JDBC database connection arguments, a list of arbitrary string tag/value. 
            Normally at least a "user" and "password" property should be included.</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Json(System.String)">
            <summary>
            Loads a JSON file (one object per line) and returns the result as a DataFrame.
            
            This function goes through the input once to determine the input schema. If you know the
            schema in advance, use the version that specifies the schema to avoid the extra scan.
            </summary>
            <param name="path">input path</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameReader.Parquet(System.String[])">
            <summary>
            Loads a Parquet file, returning the result as a [[DataFrame]]. This function returns an empty
            DataFrame if no paths are passed in.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.DataFrameWriter">
            <summary>
            Interface used to write a DataFrame to external storage systems (e.g. file systems,
            key-value stores, etc). Use DataFrame.Write to access this.
            
            See also http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Mode(Microsoft.Spark.CSharp.Sql.SaveMode)">
            <summary>
            Specifies the behavior when data or table already exists. Options include:
              - `SaveMode.Overwrite`: overwrite the existing data.
              - `SaveMode.Append`: append the data.
              - `SaveMode.Ignore`: ignore the operation (i.e. no-op).
              - `SaveMode.ErrorIfExists`: default option, throw an exception at runtime.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Mode(System.String)">
            <summary>
            Specifies the behavior when data or table already exists. Options include:
              - `SaveMode.Overwrite`: overwrite the existing data.
              - `SaveMode.Append`: append the data.
              - `SaveMode.Ignore`: ignore the operation (i.e. no-op).
              - `SaveMode.ErrorIfExists`: default option, throw an exception at runtime.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Format(System.String)">
            <summary>
            Specifies the underlying output data source. Built-in options include "parquet", "json", etc.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Option(System.String,System.String)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds output options for the underlying data source.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.PartitionBy(System.String[])">
            <summary>
            Partitions the output by the given columns on the file system. If specified, the output is
            laid out on the file system similar to Hive's partitioning scheme.
            
            This is only applicable for Parquet at the moment.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Save(System.String)">
            <summary>
            Saves the content of the DataFrame at the specified path.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Save">
            <summary>
            Saves the content of the DataFrame as the specified table.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.InsertInto(System.String)">
            <summary>
            Inserts the content of the DataFrame to the specified table. It requires that
            the schema of the DataFrame is the same as the schema of the table.
            Because it inserts data to an existing table, format or options will be ignored.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.SaveAsTable(System.String)">
            <summary>
            Saves the content of the DataFrame as the specified table.
            In the case the table already exists, behavior of this function depends on the
            save mode, specified by the `mode` function (default to throwing an exception).
            When `mode` is `Overwrite`, the schema of the DataFrame does not need to be
            the same as that of the existing table.
            When `mode` is `Append`, the schema of the DataFrame need to be
            the same as that of the existing table, and format or options will be ignored.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Jdbc(System.String,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Saves the content of the DataFrame to a external database table via JDBC. In the case the
            table already exists in the external database, behavior of this function depends on the
            save mode, specified by the `mode` function (default to throwing an exception).
            
            Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
            your external database systems.
            </summary>
            <param name="url">JDBC database url of the form `jdbc:subprotocol:subname`</param>
            <param name="table">Name of the table in the external database.</param>
            <param name="properties">JDBC database connection arguments, a list of arbitrary string tag/value. 
            Normally at least a "user" and "password" property should be included.</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Json(System.String)">
            <summary>
            Saves the content of the DataFrame in JSON format at the specified path.
            This is equivalent to:
               Format("json").Save(path)
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.DataFrameWriter.Parquet(System.String)">
            <summary>
            Saves the content of the DataFrame in JSON format at the specified path.
            This is equivalent to:
               Format("parquet").Save(path)
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.PythonSerDe">
            <summary>
            Used for SerDe of Python objects
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.PythonSerDe.GetUnpickledObjects(System.Byte[])">
            <summary>
            Unpickles objects from byte[]
            </summary>
            <param name="buffer"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.RowConstructor">
            <summary>
            Used by Unpickler to unpickle pickled objects. It is also used to construct a Row (C# representation of pickled objects).
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.RowConstructor.currentSchema">
            <summary>
            Schema of the DataFrame currently being processed
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.RowConstructor.isCurrentSchemaSet">
            <summary>
            Indicates if Schema is already set during construction of this type
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.RowConstructor.Values">
            <summary>
            Arguments used to construct this typ
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.RowConstructor.Schema">
            <summary>
            Schema of the values
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.RowConstructor.construct(System.Object[])">
            <summary>
            Used by Unpickler - do not use to construct Row. Use GetRow() method
            </summary>
            <param name="args"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.RowConstructor.GetRow">
            <summary>
            Used to construct a Row
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.Row">
            <summary>
             Represents one row of output from a relational operator.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Row.Size">
            <summary>
            Number of elements in the Row.
            </summary>
            <returns>elements count in this row</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Row.GetSchema">
            <summary>
            Schema for the row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Row.Get(System.Int32)">
            <summary>
            Returns the value at position i.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Row.Get(System.String)">
            <summary>
            Returns the value of a given columnName.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Row.GetAs``1(System.Int32)">
            <summary>
            Returns the value at position i, the return value will be cast to type T.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Row.GetAs``1(System.String)">
            <summary>
            Returns the value of a given columnName, the return value will be cast to type T.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.Functions">
            <summary>
            DataFrame Built-in functions
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Functions.Exp(Microsoft.Spark.CSharp.Sql.Column)">
            <summary>
            Computes the exponential of the given value.
            </summary>
            <param name="column"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.Functions.Expm1(Microsoft.Spark.CSharp.Sql.Column)">
            <summary>
            Computes the exponential of the given value minus one.
            </summary>
            <param name="column"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.UdfHelper`1">
            <summary>
            only used in SqlContext.RegisterFunction for now
            </summary>
            <typeparam name="RT"></typeparam>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.SaveMode">
            <summary>
            SaveMode is used to specify the expected behavior of saving a DataFrame to a data source.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.SaveMode.Append">
            <summary>
            Append mode means that when saving a DataFrame to a data source, if data/table already exists,
            contents of the DataFrame are expected to be appended to existing data.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.SaveMode.Overwrite">
            <summary>
            Overwrite mode means that when saving a DataFrame to a data source,
            if data/table already exists, existing data is expected to be overwritten by the contents of
            the DataFrame.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.SaveMode.ErrorIfExists">
            <summary>
            ErrorIfExists mode means that when saving a DataFrame to a data source, if data already exists,
            an exception is expected to be thrown.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.CSharp.Sql.SaveMode.Ignore">
            <summary>
            Ignore mode means that when saving a DataFrame to a data source, if data already exists,
            the save operation is expected to not save the contents of the DataFrame and to not
            change the existing data.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.SaveModeExtensions">
            <summary>
            For SaveMode.ErrorIfExists, the corresponding literal string in spark is "error" or "default".
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Sql.SqlContext">
            <summary>
            The entry point for working with structured data (rows and columns) in Spark.  
            Allows the creation of [[DataFrame]] objects as well as the execution of SQL queries.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.#ctor(Microsoft.Spark.CSharp.Core.SparkContext)">
            <summary>
            Creates a SqlContext
            </summary>
            <param name="sparkContext"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.GetOrCreate(Microsoft.Spark.CSharp.Core.SparkContext)">
            <summary>
            Get the existing SQLContext or create a new one with given SparkContext.
            </summary>
            <param name="sparkContext"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.NewSession">
            <summary>
            Returns a new SQLContext as new session, that has separate SQLConf, 
            registered temporary tables and UDFs, but shared SparkContext and table cache.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.GetConf(System.String,System.String)">
            <summary>
            Returns the value of Spark SQL configuration property for the given key.
            If the key is not set, returns defaultValue.
            </summary>
            <param name="key"></param>
            <param name="defaultValue"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.SetConf(System.String,System.String)">
            <summary>
            Sets the given Spark SQL configuration property.
            </summary>
            <param name="key"></param>
            <param name="value"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.Read">
            <summary>
            Returns a DataFrameReader that can be used to read data in as a DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.ReadDataFrame(System.String,Microsoft.Spark.CSharp.Sql.StructType,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Loads a dataframe the source path using the given schema and options
            </summary>
            <param name="path"></param>
            <param name="schema"></param>
            <param name="options"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.CreateDataFrame(Microsoft.Spark.CSharp.Core.RDD{System.Object[]},Microsoft.Spark.CSharp.Sql.StructType)">
            <summary>
            Creates a <see cref="T:Microsoft.Spark.CSharp.Sql.DataFrame"/> from a RDD containing array of object using the given schema.
            </summary>
            <param name="rdd">RDD containing array of object. The array acts as a row and items within the array act as columns which the schema is specified in <paramref name="schema"/>. </param>
            <param name="schema">The schema of DataFrame.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterDataFrameAsTable(Microsoft.Spark.CSharp.Sql.DataFrame,System.String)">
            <summary>
            Registers the given <see cref="T:Microsoft.Spark.CSharp.Sql.DataFrame"/> as a temporary table in the catalog.
            Temporary tables exist only during the lifetime of this instance of SqlContext.
            </summary>
            <param name="dataFrame"></param>
            <param name="tableName"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.DropTempTable(System.String)">
            <summary>
            Remove the temp table from catalog.
            </summary>
            <param name="tableName"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.Table(System.String)">
            <summary>
            Returns the specified table as a <see cref="T:Microsoft.Spark.CSharp.Sql.DataFrame"/>
            </summary>
            <param name="tableName"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.Tables(System.String)">
            <summary>
            Returns a <see cref="T:Microsoft.Spark.CSharp.Sql.DataFrame"/> containing names of tables in the given database.
            If <paramref name="databaseName"/> is not specified, the current database will be used.
            The returned DataFrame has two columns: 'tableName' and 'isTemporary' (a column with bool 
            type indicating if a table is a temporary one or not).
            </summary>
            <param name="databaseName">Name of the database to use. Default to the current database. 
            Note: This is only applicable to HiveContext.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.TableNames(System.String)">
            <summary>
            Returns a list of names of tables in the database <paramref name="databaseName"/>
            </summary>
            <param name="databaseName">Name of the database to use. Default to the current database.
            Note: This is only applicable to HiveContext.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.CacheTable(System.String)">
            <summary>
            Caches the specified table in-memory.
            </summary>
            <param name="tableName"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.UncacheTable(System.String)">
            <summary>
            Removes the specified table from the in-memory cache.
            </summary>
            <param name="tableName"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.ClearCache">
            <summary>
            Removes all cached tables from the in-memory cache.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.IsCached(System.String)">
            <summary>
            Returns true if the table is currently cached in-memory.
            </summary>
            <param name="tableName"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.Sql(System.String)">
            <summary>
            Executes a SQL query using Spark, returning the result as a DataFrame. The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'
            </summary>
            <param name="sqlQuery"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.JsonFile(System.String)">
            <summary>
            Loads a JSON file (one object per line), returning the result as a DataFrame
            It goes through the entire dataset once to determine the schema.
            </summary>
            <param name="path">path to JSON file</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.JsonFile(System.String,Microsoft.Spark.CSharp.Sql.StructType)">
            <summary>
            Loads a JSON file (one object per line) and applies the given schema
            </summary>
            <param name="path">path to JSON file</param>
            <param name="schema">schema to use</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.TextFile(System.String,Microsoft.Spark.CSharp.Sql.StructType,System.String)">
            <summary>
            Loads text file with the specific column delimited using the given schema
            </summary>
            <param name="path">path to text file</param>
            <param name="schema">schema to use</param>
            <param name="delimiter">delimiter to use</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.TextFile(System.String,System.String,System.Boolean,System.Boolean)">
            <summary>
            Loads a text file (one object per line), returning the result as a DataFrame
            </summary>
            <param name="path">path to text file</param>
            <param name="delimiter">delimited to use</param>
            <param name="hasHeader">indicates if the text file has a header row</param>
            <param name="inferSchema">indicates if every row has to be read to infer the schema; if false, columns will be strings</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``1(System.String,System.Func{``0})">
            <summary>
            Register UDF with no input argument, e.g:
                <see cref="!:SqlContext.RegisterFunction&lt;bool&gt;"/>("MyFilter", () =&gt; true);
                sqlContext.Sql("SELECT * FROM MyTable where MyFilter()");
            </summary>
            <typeparam name="RT"></typeparam>
            <param name="name"></param>
            <param name="f"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``2(System.String,System.Func{``1,``0})">
            <summary>
            Register UDF with 1 input argument, e.g:
                <see cref="!:SqlContext.RegisterFunction&lt;bool, string&gt;"/>("MyFilter", (arg1) =&gt; arg1 != null);
                sqlContext.Sql("SELECT * FROM MyTable where MyFilter(columnName1)");
            </summary>
            <typeparam name="RT"></typeparam>
            <typeparam name="A1"></typeparam>
            <param name="name"></param>
            <param name="f"></param>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``3(System.String,System.Func{``1,``2,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``4(System.String,System.Func{``1,``2,``3,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``5(System.String,System.Func{``1,``2,``3,``4,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``6(System.String,System.Func{``1,``2,``3,``4,``5,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``7(System.String,System.Func{``1,``2,``3,``4,``5,``6,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``8(System.String,System.Func{``1,``2,``3,``4,``5,``6,``7,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``9(System.String,System.Func{``1,``2,``3,``4,``5,``6,``7,``8,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``10(System.String,System.Func{``1,``2,``3,``4,``5,``6,``7,``8,``9,``0})" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.Spark.CSharp.Sql.SqlContext.RegisterFunction``11(System.String,System.Func{``1,``2,``3,``4,``5,``6,``7,``8,``9,``10,``0})" -->
        <member name="P:Microsoft.Spark.CSharp.Sql.DataType.TypeName">
            <summary>
            Trim "Type" in the end from class name, ToLower() to align with Scala.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Sql.DataType.SimpleString">
            <summary>
            return TypeName by default, subclass can override it
            </summary>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Sql.DataType.JsonValue">
            <summary>
            return only type: TypeName by default, subclass can override it
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.DStream`1">
            <summary>
            A Discretized Stream (DStream), the basic abstraction in Spark Streaming,
            is a continuous sequence of RDDs (of the same type) representing a
            continuous stream of data (see <see cref="T:Microsoft.Spark.CSharp.Core.RDD`1"/>) in the Spark core documentation
            for more details on RDDs).
            
            DStreams can either be created from live data (such as, data from TCP
            sockets, Kafka, Flume, etc.) using a <see cref="T:Microsoft.Spark.CSharp.Streaming.StreamingContext"/> or it can be
            generated by transforming existing DStreams using operations such as
            `Map`, `Window` and `ReduceByKeyAndWindow`. While a Spark Streaming
            program is running, each DStream periodically generates a RDD, either
            from live data or by transforming the RDD generated by a parent DStream.
            
            DStreams internally is characterized by a few basic properties:
             - A list of other DStreams that the DStream depends on
             - A time interval at which the DStream generates an RDD
             - A function that is used to generate an RDD after each time interval
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Count">
            <summary>
            Return a new DStream in which each RDD has a single element
            generated by counting each RDD of this DStream.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Filter(System.Func{`0,System.Boolean})">
            <summary>
            Return a new DStream containing only the elements that satisfy predicate.
            </summary>
            <param name="f"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.FlatMap``1(System.Func{`0,System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new DStream by applying a function to all elements of
            this DStream, and then flattening the results
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <param name="preservesPartitioning"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Map``1(System.Func{`0,``0},System.Boolean)">
            <summary>
            Return a new DStream by applying a function to each element of DStream.
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <param name="preservesPartitioning"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.MapPartitions``1(System.Func{System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new DStream in which each RDD is generated by applying
            mapPartitions() to each RDDs of this DStream.
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <param name="preservesPartitioning"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.MapPartitionsWithIndex``1(System.Func{System.Int32,System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new DStream in which each RDD is generated by applying
            mapPartitionsWithIndex() to each RDDs of this DStream.
            </summary>
            <typeparam name="U"></typeparam>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Reduce(System.Func{`0,`0,`0})">
            <summary>
            Return a new DStream in which each RDD has a single element
            generated by reducing each RDD of this DStream.
            </summary>
            <param name="f"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.ForeachRDD(System.Action{Microsoft.Spark.CSharp.Core.RDD{`0}})">
            <summary>
            Apply a function to each RDD in this DStream.
            </summary>
            <param name="f"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.ForeachRDD(System.Action{System.Double,Microsoft.Spark.CSharp.Core.RDD{System.Object}})">
            <summary>
            Apply a function to each RDD in this DStream.
            </summary>
            <param name="f"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Print(System.Int32)">
             <summary>
             Print the first num elements of each RDD generated in this DStream.
            
             @param num: the number of elements from the first will be printed.
             </summary>
             <param name="num"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Glom">
            <summary>
            Return a new DStream in which RDD is generated by applying glom() to RDD of this DStream.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Cache">
            <summary>
            Persist the RDDs of this DStream with the default storage level <see cref="F:Microsoft.Spark.CSharp.Core.StorageLevelType.MEMORY_ONLY_SER"/>.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Persist(Microsoft.Spark.CSharp.Core.StorageLevelType)">
            <summary>
            Persist the RDDs of this DStream with the given storage level
            </summary>
            <param name="storageLevelType"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Checkpoint(System.Int64)">
            <summary>
            Enable periodic checkpointing of RDDs of this DStream
            </summary>
            <param name="intervalMs">time in seconds, after each period of that, generated RDD will be checkpointed</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.CountByValue(System.Int32)">
            <summary>
            Return a new DStream in which each RDD contains the counts of each
            distinct value in each RDD of this DStream.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.SaveAsTextFiles(System.String,System.String)">
            <summary>
            Save each RDD in this DStream as text file, using string representation of elements.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Transform``1(System.Func{Microsoft.Spark.CSharp.Core.RDD{`0},Microsoft.Spark.CSharp.Core.RDD{``0}})">
            <summary>
            Return a new DStream in which each RDD is generated by applying a function
            on each RDD of this DStream.
            
            `func` can have one argument of `rdd`, or have two arguments of
            (`time`, `rdd`)
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Transform``1(System.Func{System.Double,Microsoft.Spark.CSharp.Core.RDD{`0},Microsoft.Spark.CSharp.Core.RDD{``0}})">
            <summary>
            Return a new DStream in which each RDD is generated by applying a function
            on each RDD of this DStream.
            
            `func` can have one argument of `rdd`, or have two arguments of
            (`time`, `rdd`)
            </summary>
            <typeparam name="U"></typeparam>
            <param name="f"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.TransformWith``2(System.Func{Microsoft.Spark.CSharp.Core.RDD{`0},Microsoft.Spark.CSharp.Core.RDD{``0},Microsoft.Spark.CSharp.Core.RDD{``1}},Microsoft.Spark.CSharp.Streaming.DStream{``0},System.Boolean)">
             <summary>
             Return a new DStream in which each RDD is generated by applying a function
             on each RDD of this DStream and 'other' DStream.
            
             `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three
             arguments of (`time`, `rdd_a`, `rdd_b`)
             </summary>
             <typeparam name="U"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="f"></param>
             <param name="other"></param>
             <param name="keepSerializer"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.TransformWith``2(System.Func{System.Double,Microsoft.Spark.CSharp.Core.RDD{`0},Microsoft.Spark.CSharp.Core.RDD{``0},Microsoft.Spark.CSharp.Core.RDD{``1}},Microsoft.Spark.CSharp.Streaming.DStream{``0},System.Boolean)">
             <summary>
             Return a new DStream in which each RDD is generated by applying a function
             on each RDD of this DStream and 'other' DStream.
            
             `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three
             arguments of (`time`, `rdd_a`, `rdd_b`)
             </summary>
             <typeparam name="U"></typeparam>
             <typeparam name="V"></typeparam>
             <param name="f"></param>
             <param name="other"></param>
             <param name="keepSerializer"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Repartition(System.Int32)">
            <summary>
            Return a new DStream with an increased or decreased level of parallelism.
            </summary>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Union(Microsoft.Spark.CSharp.Streaming.DStream{`0})">
             <summary>
             Return a new DStream by unifying data of another DStream with this DStream.
            
             @param other: Another DStream having the same interval (i.e., slideDuration) as this DStream.
             </summary>
             <param name="other"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Slice(System.DateTime,System.DateTime)">
            <summary>
            Return all the RDDs between 'fromTime' to 'toTime' (both included)
            </summary>
            <param name="fromTimeUtc"></param>
            <param name="toTimeUtc"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.Window(System.Int32,System.Int32)">
             <summary>
             Return a new DStream in which each RDD contains all the elements in seen in a
             sliding window of time over this DStream.
            
             @param windowDuration: width of the window; must be a multiple of this DStream's
                                  batching interval
             @param slideDuration:  sliding interval of the window (i.e., the interval after which
                                  the new DStream will generate RDDs); must be a multiple of this
                                  DStream's batching interval
             </summary>
             <param name="windowSeconds"></param>
             <param name="slideSeconds"></param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.ReduceByWindow(System.Func{`0,`0,`0},System.Func{`0,`0,`0},System.Int32,System.Int32)">
             <summary>
             Return a new DStream in which each RDD has a single element generated by reducing all
             elements in a sliding window over this DStream.
            
             if `invReduceFunc` is not None, the reduction is done incrementally
             using the old window's reduced value :
            
             1. reduce the new values that entered the window (e.g., adding new counts)
            
             2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)
             This is more efficient than `invReduceFunc` is None.
            
             </summary>
             <param name="reduceFunc">associative reduce function</param>
             <param name="invReduceFunc">inverse reduce function of `reduceFunc`</param>
             <param name="windowSeconds">width of the window; must be a multiple of this DStream's batching interval</param>
             <param name="slideSeconds">sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval</param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.CountByWindow(System.Int32,System.Int32)">
            <summary>
            Return a new DStream in which each RDD has a single element generated
            by counting the number of elements in a window over this DStream.
            windowDuration and slideDuration are as defined in the window() operation.
            
            This is equivalent to window(windowDuration, slideDuration).count(),
            but will be more efficient if window is large.
            </summary>
            <param name="windowSeconds"></param>
            <param name="slideSeconds"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.DStream`1.CountByValueAndWindow(System.Int32,System.Int32,System.Int32)">
            <summary>
            Return a new DStream in which each RDD contains the count of distinct elements in
            RDDs in a sliding window over this DStream.
            </summary>
            <param name="windowSeconds">width of the window; must be a multiple of this DStream's batching interval</param>
            <param name="slideSeconds">
                sliding interval of the window (i.e., the interval after which
                the new DStream will generate RDDs); must be a multiple of this
                DStream's batching interval        
            </param>
            <param name="numPartitions">number of partitions of each RDD in the new DStream.</param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.Spark.CSharp.Streaming.DStream`1.SlideDuration">
            <summary>
            Return the slideDuration in seconds of this DStream
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.MapPartitionsWithIndexHelper`2">
            <summary>
            Following classes are defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not marked serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. These classes are to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.EventHubsUtils">
            <summary>
            Utility for creating streams from 
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.EventHubsUtils.CreateUnionStream(Microsoft.Spark.CSharp.Streaming.StreamingContext,System.Collections.Generic.Dictionary{System.String,System.String},Microsoft.Spark.CSharp.Core.StorageLevelType)">
             <summary>
             Create a unioned EventHubs stream that receives data from Microsoft Azure Eventhubs
             The unioned stream will receive message from all partitions of the EventHubs
             </summary>
             <param name="ssc">Streaming context</param>
             <param name="eventhubsParams"> Parameters for EventHubs.
              Required parameters are:
              "eventhubs.policyname": EventHubs policy name
              "eventhubs.policykey": EventHubs policy key
              "eventhubs.namespace": EventHubs namespace
              "eventhubs.name": EventHubs name
              "eventhubs.partition.count": Number of partitions
              "eventhubs.checkpoint.dir": checkpoint directory on HDFS
            
              Optional parameters are:
              "eventhubs.consumergroup": EventHubs consumer group name, default to "\$default"
              "eventhubs.filter.offset": Starting offset of EventHubs, default to "-1"
              "eventhubs.filter.enqueuetime": Unix time, millisecond since epoch, default to "0"
              "eventhubs.default.credits": default AMQP credits, default to -1 (which is 1024)
              "eventhubs.checkpoint.interval": checkpoint interval in second, default to 10
             </param>
             <param name="storageLevelType">Storage level, by default it is MEMORY_ONLY</param>
             <returns>DStream with byte[] representing events from EventHub</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.KafkaUtils.CreateStream(Microsoft.Spark.CSharp.Streaming.StreamingContext,System.String,System.String,System.Collections.Generic.Dictionary{System.String,System.Int32},System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Create an input stream that pulls messages from a Kafka Broker.
            </summary>
            <param name="ssc">Spark Streaming Context</param>
            <param name="zkQuorum">Zookeeper quorum (hostname:port,hostname:port,..).</param>
            <param name="groupId">The group id for this consumer.</param>
            <param name="topics">Dict of (topic_name -> numPartitions) to consume. Each partition is consumed in its own thread.</param>
            <param name="kafkaParams">Additional params for Kafka</param>
            <returns>A DStream object</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.KafkaUtils.CreateStream(Microsoft.Spark.CSharp.Streaming.StreamingContext,System.String,System.String,System.Collections.Generic.Dictionary{System.String,System.Int32},System.Collections.Generic.Dictionary{System.String,System.String},Microsoft.Spark.CSharp.Core.StorageLevelType)">
            <summary>
            Create an input stream that pulls messages from a Kafka Broker.
            </summary>
            <param name="zkQuorum">Zookeeper quorum (hostname:port,hostname:port,..).</param>
            <param name="groupId">The group id for this consumer.</param>
            <param name="topics">Dict of (topic_name -> numPartitions) to consume. Each partition is consumed in its own thread.</param>
            <param name="kafkaParams">Additional params for Kafka</param>
            <param name="storageLevelType">RDD storage level.</param>
            <returns>A DStream object</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.KafkaUtils.CreateDirectStream(Microsoft.Spark.CSharp.Streaming.StreamingContext,System.Collections.Generic.List{System.String},System.Collections.Generic.Dictionary{System.String,System.String},System.Collections.Generic.Dictionary{System.String,System.Int64})">
            <summary>
            Create an input stream that directly pulls messages from a Kafka Broker and specific offset.
            
            This is not a receiver based Kafka input stream, it directly pulls the message from Kafka
            in each batch duration and processed without storing.
            
            This does not use Zookeeper to store offsets. The consumed offsets are tracked
            by the stream itself. For interoperability with Kafka monitoring tools that depend on
            Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.
            You can access the offsets used in each batch from the generated RDDs (see
            [[org.apache.spark.streaming.kafka.HasOffsetRanges]]).
            To recover from driver failures, you have to enable checkpointing in the StreamingContext.
            The information on consumed offset can be recovered from the checkpoint.
            See the programming guide for details (constraints, etc.).
            
            </summary>
            <param name="ssc">Spark Streaming Context</param>
            <param name="topics">list of topic_name to consume.</param>
            <param name="kafkaParams">
                Additional params for Kafka. Requires "metadata.broker.list" or "bootstrap.servers" to be set
                with Kafka broker(s) (NOT zookeeper servers), specified in host1:port1,host2:port2 form.        
            </param>
            <param name="fromOffsets">Per-topic/partition Kafka offsets defining the (inclusive) starting point of the stream.</param>
            <returns>A DStream object</returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.KafkaUtils.CreateDirectStreamWithRepartition(Microsoft.Spark.CSharp.Streaming.StreamingContext,System.Collections.Generic.List{System.String},System.Collections.Generic.Dictionary{System.String,System.String},System.Collections.Generic.Dictionary{System.String,System.Int64},System.UInt32)">
            <summary>
            Create an input stream that directly pulls messages from a Kafka Broker and specific offset.
            
            This is not a receiver based Kafka input stream, it directly pulls the message from Kafka
            in each batch duration and processed without storing.
            
            This does not use Zookeeper to store offsets. The consumed offsets are tracked
            by the stream itself. For interoperability with Kafka monitoring tools that depend on
            Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.
            You can access the offsets used in each batch from the generated RDDs (see
            [[org.apache.spark.streaming.kafka.HasOffsetRanges]]).
            To recover from driver failures, you have to enable checkpointing in the StreamingContext.
            The information on consumed offset can be recovered from the checkpoint.
            See the programming guide for details (constraints, etc.).
            
            </summary>
            <param name="ssc">Spark Streaming Context</param>
            <param name="topics">list of topic_name to consume.</param>
            <param name="kafkaParams">
                Additional params for Kafka. Requires "metadata.broker.list" or "bootstrap.servers" to be set
                with Kafka broker(s) (NOT zookeeper servers), specified in host1:port1,host2:port2 form.        
            </param>
            <param name="fromOffsets">Per-topic/partition Kafka offsets defining the (inclusive) starting point of the stream.</param>
            <param name="numPartitions">
                user hint on how many kafka RDD partitions to create instead of aligning with kafka partitions,
                unbalanced kafka partitions and/or under-distributed data will be redistributed evenly across 
                a probably larger number of RDD partitions
            </param>
            <returns>A DStream object</returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.MapWithStateDStream`4">
            <summary>
            DStream representing the stream of data generated by `mapWithState` operation on a pair DStream.
            Additionally, it also gives access to the stream of state snapshots, that is, the state data of all keys after a batch has updated them.
            </summary>
            <typeparam name="K">Type of the key</typeparam>
            <typeparam name="V">Type of the value</typeparam>
            <typeparam name="S">Type of the state data</typeparam>
            <typeparam name="M">Type of the mapped data</typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.MapWithStateDStream`4.StateSnapshots">
            <summary>
            Return a pair DStream where each RDD is the snapshot of the state of all the keys.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.KeyedState`1">
            <summary>
            Class to hold a state instance and the timestamp when the state is updated or created.
            No need to explicitly make this class clonable, since the serialization and deserialization in Worker is already a kind of clone mechanism. 
            </summary>
            <typeparam name="S">Type of the state data</typeparam>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.MapWithStateRDDRecord`3">
            <summary>
            Record storing the keyed-state MapWithStateRDD. 
            Each record contains a stateMap and a sequence of records returned by the mapping function of MapWithState.
            Note: don't need to explicitly make this class clonable, since the serialization and deserialization in Worker is already a kind of clone. 
            </summary>
            <typeparam name="K">Type of the key</typeparam>
            <typeparam name="S">Type of the state data</typeparam>
            <typeparam name="M">Type of the mapped data</typeparam>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.UpdateStateHelper`4">
            <summary>
            Helper class to update states for a RDD partition.
            Reference: https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala
            </summary>
            <typeparam name="K">Type of the key</typeparam>
            <typeparam name="V">Type of the value</typeparam>
            <typeparam name="S">Type of the state data</typeparam>
            <typeparam name="M">Type of the mapped data</typeparam>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.StateSpec`4">
            <summary>
            Representing all the specifications of the DStream transformation `mapWithState` operation.
            </summary>
            <typeparam name="K">Type of the key</typeparam>
            <typeparam name="V">Type of the value</typeparam>
            <typeparam name="S">Type of the state data</typeparam>
            <typeparam name="M">Type of the mapped data</typeparam>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StateSpec`4.#ctor(System.Func{`0,`1,Microsoft.Spark.CSharp.Streaming.State{`2},`3})">
            <summary>
            Create a StateSpec for setting all the specifications of the `mapWithState` operation on a pair DStream.
            </summary>
            <param name="mappingFunction">The function applied on every data item to manage the associated state and generate the mapped data</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StateSpec`4.NumPartitions(System.Int32)">
            <summary>
            Set the number of partitions by which the state RDDs generated by `mapWithState` will be partitioned.
            Hash partitioning will be used.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StateSpec`4.Timeout(System.TimeSpan)">
            <summary>
            Set the duration after which the state of an idle key will be removed. A key and its state is
            considered idle if it has not received any data for at least the given duration. The
            mapping function will be called one final time on the idle states that are going to be
            removed; [[org.apache.spark.streaming.State State.isTimingOut()]] set to `true` in that call.
            </summary>
            <param name="ts"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.State`1">
            <summary>
            class for getting and updating the state in mapping function used in the `mapWithState` operation
            </summary>
            <typeparam name="S">Type of the state</typeparam>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions">
            <summary>
            operations only available to KeyValuePair RDD
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.ReduceByKey``2(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,``1,``1},System.Int32)">
            <summary>
            Return a new DStream by applying ReduceByKey to each RDD.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="reduceFunc"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.CombineByKey``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``2},System.Func{``2,``1,``2},System.Func{``2,``2,``2},System.Int32)">
            <summary>
            Return a new DStream by applying combineByKey to each RDD.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="C"></typeparam>
            <param name="self"></param>
            <param name="createCombiner"></param>
            <param name="mergeValue"></param>
            <param name="mergeCombiners"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.PartitionBy``2(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Int32)">
            <summary>
            Return a new DStream in which each RDD are partitioned by numPartitions.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.MapValues``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,``2})">
            <summary>
            Return a new DStream by applying a map function to the value of
            each key-value pairs in this DStream without changing the key.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="U"></typeparam>
            <param name="self"></param>
            <param name="func"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.FlatMapValues``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,System.Collections.Generic.IEnumerable{``2}})">
            <summary>
            Return a new DStream by applying a flatmap function to the value
            of each key-value pairs in this DStream without changing the key.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="U"></typeparam>
            <param name="self"></param>
            <param name="func"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.GroupByKey``2(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Int32)">
            <summary>
            Return a new DStream by applying groupByKey on each RDD.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.GroupWith``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Return a new DStream by applying 'cogroup' between RDDs of this DStream and `other` DStream.
            Hash partitioning is used to generate the RDDs with `numPartitions` partitions.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.Join``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Return a new DStream by applying 'join' between RDDs of this DStream and `other` DStream.
            Hash partitioning is used to generate the RDDs with `numPartitions` partitions.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.LeftOuterJoin``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Return a new DStream by applying 'left outer join' between RDDs of this DStream and `other` DStream.
            Hash partitioning is used to generate the RDDs with `numPartitions` partitions.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.RightOuterJoin``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Return a new DStream by applying 'right outer join' between RDDs of this DStream and `other` DStream.
            Hash partitioning is used to generate the RDDs with `numPartitions` partitions.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.FullOuterJoin``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``2}},System.Int32)">
            <summary>
            Return a new DStream by applying 'full outer join' between RDDs of this DStream and `other` DStream.
            Hash partitioning is used to generate the RDDs with `numPartitions` partitions.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="W"></typeparam>
            <param name="self"></param>
            <param name="other"></param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.GroupByKeyAndWindow``2(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Int32,System.Int32,System.Int32)">
            <summary>
            Return a new DStream by applying `GroupByKey` over a sliding window.
            Similar to `DStream.GroupByKey()`, but applies it over a sliding window.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <param name="self"></param>
            <param name="windowSeconds">width of the window; must be a multiple of this DStream's batching interval</param>
            <param name="slideSeconds">
                sliding interval of the window (i.e., the interval after which
                the new DStream will generate RDDs); must be a multiple of this
                DStream's batching interval
            </param>
            <param name="numPartitions">Number of partitions of each RDD in the new DStream.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.ReduceByKeyAndWindow``2(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{``1,``1,``1},System.Func{``1,``1,``1},System.Int32,System.Int32,System.Int32,System.Func{System.Collections.Generic.KeyValuePair{``0,``1},System.Boolean})">
             <summary>
             Return a new DStream by applying incremental `reduceByKey` over a sliding window.
            
             The reduced value of over a new window is calculated using the old window's reduce value :
              1. reduce the new values that entered the window (e.g., adding new counts)
              2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)
            
             `invFunc` can be None, then it will reduce all the RDDs in window, could be slower than having `invFunc`.
             </summary>
             <typeparam name="K"></typeparam>
             <typeparam name="V"></typeparam>
             <param name="self"></param>
             <param name="reduceFunc">associative reduce function</param>
             <param name="invReduceFunc">inverse function of `reduceFunc`</param>
             <param name="windowSeconds">width of the window; must be a multiple of this DStream's batching interval</param>
             <param name="slideSeconds">sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval</param>
             <param name="numPartitions">number of partitions of each RDD in the new DStream.</param>
             <param name="filterFunc">function to filter expired key-value pairs; only pairs that satisfy the function are retained set this to null if you do not want to filter</param>
             <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.UpdateStateByKey``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{System.Collections.Generic.IEnumerable{``1},``2,``2},System.Int32)">
            <summary>
            Return a new "state" DStream where the state for each key is updated by applying
            the given function on the previous state of the key and the new values of the key.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="S"></typeparam>
            <param name="self"></param>
            <param name="updateFunc">
                State update function - (newValues, oldState) => newState
                If this function returns None, then corresponding state key-value pair will be eliminated.
            </param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.UpdateStateByKey``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{``0,System.Tuple{System.Collections.Generic.IEnumerable{``1},``2}}},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{``0,``2}}},System.Int32)">
            <summary>
            Return a new "state" DStream where the state for each key is updated by applying
            the given function on the previous state of the key and the new values of the key.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="S"></typeparam>
            <param name="self"></param>
            <param name="updateFunc">State update function - IEnumerable[K, [newValues, oldState]] => IEnumerable[K, newState]</param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.UpdateStateByKey``3(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},System.Func{System.Int32,System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{``0,System.Tuple{System.Collections.Generic.IEnumerable{``1},``2}}},System.Collections.Generic.IEnumerable{System.Collections.Generic.KeyValuePair{``0,``2}}},System.Int32)">
            <summary>
            Return a new "state" DStream where the state for each key is updated by applying
            the given function on the previous state of the key and the new values of the key.
            </summary>
            <typeparam name="K"></typeparam>
            <typeparam name="V"></typeparam>
            <typeparam name="S"></typeparam>
            <param name="self"></param>
            <param name="updateFunc">State update function - (pid, IEnumerable[K, [newValues, oldState]]) => IEnumerable[K, newState]</param>
            <param name="numPartitions"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.PairDStreamFunctions.MapWithState``4(Microsoft.Spark.CSharp.Streaming.DStream{System.Collections.Generic.KeyValuePair{``0,``1}},Microsoft.Spark.CSharp.Streaming.StateSpec{``0,``1,``2,``3})">
            <summary>
            Return a new "state" DStream where the state for each key is updated by applying
            the given function on the previous state of the key and the new values of the key.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.CombineByKeyHelper`3">
            <summary>
            Following classes are defined explicitly instead of using anonymous method as delegate to prevent C# compiler from generating
            private anonymous type that is not marked serializable. Since the delegate has to be serialized and sent to the Spark workers
            for execution, it is necessary to have the type marked [Serializable]. These classes are to work around the limitation
            on the serializability of compiler generated types
            </summary>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.StreamingContext">
            Main entry point for Spark Streaming functionality. It provides methods used to create
            [[org.apache.spark.streaming.dstream.DStream]]s from various input sources. It can be either
            created by providing a Spark master URL and an appName, or from a org.apache.spark.SparkConf
            configuration (see core Spark documentation), or from an existing org.apache.spark.SparkContext.
            The associated SparkContext can be accessed using `context.sparkContext`. After
            creating and transforming DStreams, the streaming computation can be started and stopped
            using `context.start()` and `context.stop()`, respectively.
            `context.awaitTermination()` allows the current thread to wait for the termination
            of the context by `stop()` or by an exception.
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.#ctor(Microsoft.Spark.CSharp.Proxy.IStreamingContextProxy)">
            <summary>
            when created from checkpoint
            </summary>
            <param name="streamingContextProxy"></param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.GetOrCreate(System.String,System.Func{Microsoft.Spark.CSharp.Streaming.StreamingContext})">
            <summary>
            Either recreate a StreamingContext from checkpoint data or create a new StreamingContext.
            If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be
            recreated from the checkpoint data. If the data does not exist, then the provided setupFunc
            will be used to create a JavaStreamingContext.
            </summary>
            <param name="checkpointPath">Checkpoint directory used in an earlier JavaStreamingContext program</param>
            <param name="creatingFunc">Function to create a new JavaStreamingContext and setup DStreams</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.Remember(System.Int64)">
            <summary>
            Set each DStreams in this context to remember RDDs it generated in the last given duration.
            DStreams remember RDDs only for a limited duration of time and releases them for garbage
            collection. This method allows the developer to specify how long to remember the RDDs (
            if the developer wishes to query old data outside the DStream computation).
            </summary>
            <param name="durationMs">Minimum duration that each DStream should remember its RDDs</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.Checkpoint(System.String)">
            <summary>
            Set the context to periodically checkpoint the DStream operations for driver
            fault-tolerance.
            </summary>
            <param name="directory">
                HDFS-compatible directory where the checkpoint data will be reliably stored. 
                Note that this must be a fault-tolerant file system like HDFS for
            </param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.SocketTextStream(System.String,System.Int32,Microsoft.Spark.CSharp.Core.StorageLevelType)">
            <summary>
            Create an input from TCP source hostname:port. Data is received using
            a TCP socket and receive byte is interpreted as UTF8 encoded ``\\n`` delimited
            lines.
            </summary>
            <param name="hostname">Hostname to connect to for receiving data</param>
            <param name="port">Port to connect to for receiving data</param>
            <param name="storageLevelType">Storage level to use for storing the received objects</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.TextFileStream(System.String)">
            <summary>
            Create an input stream that monitors a Hadoop-compatible file system
            for new files and reads them as text files. Files must be wrriten to the
            monitored directory by "moving" them from another location within the same
            file system. File names starting with . are ignored.
            </summary>
            <param name="directory"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.AwaitTermination">
            <summary>
            Wait for the execution to stop.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.AwaitTerminationOrTimeout(System.Int32)">
            <summary>
            Wait for the execution to stop.
            </summary>
            <param name="timeout">time to wait in seconds</param>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.Transform``1(System.Collections.Generic.IEnumerable{Microsoft.Spark.CSharp.Streaming.DStream{``0}},System.Func{System.Collections.Generic.IEnumerable{Microsoft.Spark.CSharp.Core.RDD{``0}},System.Int64,Microsoft.Spark.CSharp.Core.RDD{``0}})">
            <summary>
            Create a new DStream in which each RDD is generated by applying
            a function on RDDs of the DStreams. The order of the JavaRDDs in
            the transform function parameter will be the same as the order
            of corresponding DStreams in the list.
            </summary>
            <typeparam name="T"></typeparam>
            <param name="dstreams"></param>
            <param name="transformFunc"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.CSharp.Streaming.StreamingContext.Union``1(Microsoft.Spark.CSharp.Streaming.DStream{``0}[])">
            <summary>
            Create a unified DStream from multiple DStreams of the same
            type and same slide duration.
            </summary>
            <typeparam name="T"></typeparam>
            <param name="dstreams"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.CSharp.Streaming.TransformedDStream`1">
            <summary>
            TransformedDStream is an DStream generated by an C# function
            transforming each RDD of an DStream to another RDDs.
            
            Multiple continuous transformations of DStream can be combined into
            one transformation.
            </summary>
            <typeparam name="U"></typeparam>
        </member>
    </members>
</doc>
